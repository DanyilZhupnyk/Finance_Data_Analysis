{"cells":[{"cell_type":"markdown","id":"6957a32a-b61d-4371-9c9f-d7d9f778bacf","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"400\" alt=\"cognitiveclass.ai logo\"\u003e\n","\u003c/center\u003e\n","\n","# **Investigation relationships between exchange rate BTC/BUSD and ADOSC, NATR, TRANGE indicators**\n","\n","\n","## Lab 2. Data Wrangling\n","\n","\n","Estimated time needed: **30** minutes\n","\n","\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \n","Для Марії\n","### The tasks:\n","*   \n","\n","\u003c/div\u003e\n","\n","### Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Handle missing values\n","*   Correct data format\n","*   Standardize and normalize data\n","*   Resample data\n","\n","\u003ch3\u003eTable of Contents\u003c/h3\u003e\n","\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","\u003col\u003e\n","    \u003cli\u003eImport and Load Data\u003c/li\u003e\n","    \u003cli\u003eGenerating missing values\u003c/li\u003e\n","    \u003cli\u003eIdentify and handle missing values\n","        \u003cul\u003e\n","            \u003cli\u003eIdentify missing values\u003c/li\u003e\n","            \u003cli\u003eDeal with missing values\u003c/li\u003e\n","            \u003cli\u003eCorrect data format\u003c/li\u003e\n","        \u003c/ul\u003e\n","    \u003c/li\u003e\n","    \u003cli\u003eData standardization\u003c/li\u003e\n","    \u003cli\u003eData normalization (centering/scaling)\u003c/li\u003e\n","    \u003cli\u003eBinning\u003c/li\u003e\n","    \u003cli\u003eIndicator variable\u003c/li\u003e\n","    \u003cli\u003eResampling\u003c/li\u003e\n","\u003c/ol\u003e\n","\n","\u003c/div\u003e\n"]},{"cell_type":"markdown","id":"c1716163-c9f4-430b-8a10-dc19e511a67c","metadata":{},"outputs":[],"source":["## Dataset Description\n","\n","### Context\n","Dataset contains historical changes of the ***BTC/BUSD*** and ***ADOSC, NATR, TRANGE indicators*** for the period from *11/11/2022 to 11/24/2022* with an *1-minute* aggregation time.\n","\n","### Columns\n","\n","#### Input columns\n","* ***Time*** - the timestamp of the record\n","* ***Open*** -  the price of the asset at the beginning of the trading period\n","* ***High*** -  the highest price of the asset during the trading period\n","* ***Low*** - the lowest price of the asset during the trading period.\n","* ***Close*** - the price of the asset at the end of the trading period\n","* ***Volume*** - the total number of shares or contracts of a particular asset that are traded during a given period\n","* ***Count*** -  the number of individual trades or transactions that have been executed during a given time period\n","* ***ADOSC*** - Chaikin oscillator indicator\n","* ***NATR*** - normalized average true range (ATR) indicator\n","* ***TRANGE*** - true range indicator\n","\n","#### Target column\n","* ***Price*** - the average price at which a particular asset has been bought or sold during a given period\n"]},{"cell_type":"markdown","id":"78bc01e9-d45d-4389-bfce-a7b101fc3ae7","metadata":{},"outputs":[],"source":["## 1. Import and Load Data\n","\n","### Import data\n","\u003cp\u003e\n","You can find the dataset from the following \u003ca href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08LZES/BTCBUSD_trades_1m.csv\"\u003elink\u003c/a\u003e. We will be using this dataset throughout this course.\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"1656273d-2b17-4198-893f-263bd3256431","metadata":{},"outputs":[],"source":["If you run the lab locally using Anaconda, you can load the correct library and versions by uncommenting the following:\n"]},{"cell_type":"code","id":"110b8622-f573-4e80-a9af-d75502c1a8f5","metadata":{},"outputs":[],"source":["#If you run the lab locally using Anaconda, you can load the correct library and versions by uncommenting the following:\n#install specific version of libraries used in lab\n#! mamba install pandas==1.3.3\n#! mamba install numpy=1.21.2\n#! mamba install scipy=1.7.1-y\n#! mamba install scikit-learn"]},{"cell_type":"markdown","id":"58478893-87ba-4b3d-9409-b2178a548ec0","metadata":{},"outputs":[],"source":["Let's import the modules we will use:\n"]},{"cell_type":"code","id":"b69839c7-5c31-4db0-b2f7-455476eedf34","metadata":{},"outputs":[],"source":["import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport requests\nimport json\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nimport random\nimport string\nimport sklearn.metrics\nimport matplotlib.pylab as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n%matplotlib inline\nimport matplotlib as plt\nfrom matplotlib import pyplot"]},{"cell_type":"markdown","id":"4d5d742e-c67c-4ab1-b740-e3d398076083","metadata":{},"outputs":[],"source":["### Read Dataset\n","\n","First, we assign the URL of the dataset to \u003ccode\u003e\"path\"\u003c/code\u003e.\n","\n","This dataset was hosted on IBM Cloud object. Click \u003ca href=\"https://cocl.us/corsera_da0101en_notebook_bottom?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2021-01-01\"\u003eHERE\u003c/a\u003e for free storage.\n"]},{"cell_type":"code","id":"e95cc973-a465-48c1-ac4a-8abc106334f9","metadata":{},"outputs":[],"source":["path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08LZES/BTCBUSD_trades_1m.csv\""]},{"cell_type":"markdown","id":"9f10f564-a916-4497-a168-0c105d01c96b","metadata":{},"outputs":[],"source":["Use the Pandas method \u003ccode\u003eread_csv()\u003c/code\u003e to load the data from the web address. Set the parameter  \u003ccode\u003eindex_col=0\u003c/code\u003e in order to use the first column of cvs file as the index of the dataframe.\n"]},{"cell_type":"code","id":"9a7a7bc7-e585-4f37-ae99-b0f733ab7022","metadata":{},"outputs":[],"source":["df = pd.read_csv(path, index_col=0)"]},{"cell_type":"markdown","id":"9151d4bf-7815-4c7e-97af-eba9e03c6cc1","metadata":{},"outputs":[],"source":["In finance you sometimes need to use different numbers of decimal places. For ease of reading, let's specify the value of the precision parameter equal to 4 to display three decimal signs (instead of 6 as default).\n"]},{"cell_type":"code","id":"29c0200e-ba1b-43a5-8fad-15a9313262ea","metadata":{},"outputs":[],"source":["pd.set_option(\"display.precision\", 4)"]},{"cell_type":"markdown","id":"39de26bd-4707-4d79-8f29-e3d2bb663258","metadata":{},"outputs":[],"source":["Set dataframe index column type to \u003cstrong\u003edatetime\u003c/strong\u003e using \u003ccode\u003epd.to_datetime()\u003c/code\u003e method for correct time series analysis. \n"]},{"cell_type":"code","id":"322689ef-43dc-4899-b657-ac1041783ad9","metadata":{},"outputs":[],"source":["df.index = pd.to_datetime(df.index)"]},{"cell_type":"markdown","id":"55df0de9-08e6-42b3-a867-3c2976157ca9","metadata":{},"outputs":[],"source":["Use the method \u003ccode\u003ehead()\u003c/code\u003e to display the first five rows of the dataframe.\n"]},{"cell_type":"code","id":"c557554b-f054-44a6-ab67-f04521d3c7a5","metadata":{},"outputs":[],"source":["# To see what the data set looks like, we'll use the head() method.\ndf.head()"]},{"cell_type":"markdown","id":"225a5966-bd61-4f79-8a68-89197b67bebb","metadata":{},"outputs":[],"source":["### Drop NaN\n"]},{"cell_type":"markdown","id":"f8ece997-508b-4f5b-8e92-7a8aebf7e55a","metadata":{},"outputs":[],"source":["In the previous lab we calculated technical financial indicators. Since the values of previous periods had to be taken into account for their calculation, the first few lines of the dataframe contain `NaN` values.\n","\n","We will use different methods for recovering missing data in this module that do not work correctly with recovering data in the first rows of time series. Therefore, we need to remove `NaN` values with `df.dropna(inplace=True)` method.\n"]},{"cell_type":"code","id":"3a097f85-d121-4da0-b93b-f95e7dbfce3d","metadata":{},"outputs":[],"source":["df.dropna(inplace=True)\ndf.head()"]},{"cell_type":"markdown","id":"40d075bd-97be-49e5-81d0-1a114476adee","metadata":{},"outputs":[],"source":["Great! Now we're ready to get started and get familiar with data wrangling.\n"]},{"cell_type":"markdown","id":"0db320d9-60b1-4885-aa48-7644dc0e86d9","metadata":{},"outputs":[],"source":["### What is the purpose of data wrangling?\n","\n","\u003cstrong\u003e\u003cem\u003eData wrangling\u003c/em\u003e\u003c/strong\u003e is the process of converting data from the initial format to a format that may be better for analysis.\n"]},{"cell_type":"markdown","id":"0f7d988d-9730-4996-b50b-b4f0f51b7761","metadata":{},"outputs":[],"source":["## 2. Generating missing values\n"]},{"cell_type":"markdown","id":"318751d5-4787-4d15-8a8e-4eec2b510474","metadata":{},"outputs":[],"source":["One of the important steps of data wrangling is identifying gaps or empty cells in data and either filling or removing them. \n","\u003cp\u003e\n","Let's find out if our dataset has fields with missing values. Use method \u003ccode\u003eis.null()\u003c/code\u003e to detect missing values. The output is a boolean value indicating whether the value is in fact missing data. \u003cstrong\u003eFalse\u003c/strong\u003e means the cell is not empty, \u003cstrong\u003eTrue\u003c/strong\u003e indicates missing value.\n","\u003c/p\u003e\n"]},{"cell_type":"raw","id":"071d922c-aea5-48a7-997a-15c7f8d4fd39","metadata":{},"outputs":[],"source":["dataframe.isnull()"]},{"cell_type":"code","id":"2833f163-eb53-4a26-8c16-07a4b0af2c53","metadata":{},"outputs":[],"source":["missing_data = df.isnull()\nmissing_data.head()"]},{"cell_type":"markdown","id":"2952435a-5f69-41c5-83a0-a3c0be44f9b8","metadata":{},"outputs":[],"source":["After that use \u003ccode\u003evalues_counts()\u003c/code\u003e to return a series containing counts of unique rows in the dataframe.\n"]},{"cell_type":"raw","id":"375bc141-14eb-4ab5-80df-3f864a613703","metadata":{},"outputs":[],"source":["dataframe.value_counts()"]},{"cell_type":"code","id":"bcd09282-cc89-4830-b7a1-9b5def33faf4","metadata":{},"outputs":[],"source":["missing_data.value_counts()"]},{"cell_type":"markdown","id":"bcf324da-2c43-49c9-b017-9fd2be3b94e3","metadata":{},"outputs":[],"source":["As we can see each column of our dataframe has only boolean **False** value that indicates no missing values are present in our dataset. \n","\n","\u003cp\u003e\n","To gain a better experience of each steps of data wrangling process let's generate missing values in our dataset. Then we will try to restore them and compare with a real data.\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"e2f90e9c-9c80-424d-950a-ac8ab331fcd5","metadata":{},"outputs":[],"source":["### Generating incorrect data\n"]},{"cell_type":"markdown","id":"061e7f41-fec2-4947-9dac-7aa8be7b66ae","metadata":{},"outputs":[],"source":["Missing data is not the only incorrect data that may occur in datasets. Let us generate the following incorrect data for \u003cem\u003eOHLCV parameters (Open, High, Low, Close, Volume)\u003c/em\u003e within our dataset:\n","\u003cli\u003e \u003cem\u003emissing values (NaN)\u003c/em\u003e \u003c/li\u003e\n","\u003cli\u003e \u003cem\u003enegative values\u003c/em\u003e \u003c/li\u003e\n","\u003cli\u003e \u003cem\u003estrings\u003c/em\u003e \u003c/li\u003e\n","\u003cbr\u003e\n","\u003cp\u003e\n","We should declare function \u003ccode\u003egenerate_incorrect_data()\u003c/code\u003e responsible of this task.\n","\u003c/p\u003e\n","\u003cp\u003e\n","In order to generate random values, the \u003ccode\u003erand()\u003c/code\u003e method of random module and numpy \u003ccode\u003ewhere()\u003c/code\u003e function will be used.\n","\u003c/p\u003e    \n","\u003cp\u003e\n","Here we use the function: \n"," \u003cpre\u003enumpy.where(condition, x, y)\u003c/pre\u003e\n","\n","to return elements chosen from x or y depending on condition.\n","\u003c/p\u003e\n","\u003cp\u003e\n","    In our instance specify a condition \u003ccode\u003enp.random.rand(len(df))\u003c/code\u003e greater than 0.07 to generate part of the incorrect dataset data. If condition equals True, then yield x, otherwise yield incorrect data such as NaN, strings, and negative values.\n","\u003c/p\u003e\n","\u003cp\u003e\n","    To generate negative values we use \u003ccode\u003enp.random.uniform(low, high)\u003c/code\u003e that draws samples from a uniform distribution over \u003ccode\u003e[low, high)\u003c/code\u003e interval.\u003c/code\u003e\n","\u003c/p\u003e\n","\u003cp\u003e\n","    To generate strings we use \u003ccode\u003erandom.choices(array, k)\u003c/code\u003e to create a random sample from a given array. In our case we specify array as ASCII letters \u003cem\u003estring.ascii_letters\u003c/em\u003e and create sequences of letters with a length of 7 characters \u003ccode\u003ek=7\u003c/code\u003e. As the \u003ccode\u003echoices()\u003c/code\u003e method returns a list with the randomly selected element from the specified array, we need to convert it to \u003cem\u003estring\u003c/em\u003e type. To accomplish this we use the \u003ccode\u003estring.join()\u003c/code\u003e method that takes all items in an iterable and joins them into one string.\n","\u003c/p\u003e\n"]},{"cell_type":"code","id":"c591d45e-fe8b-4cf1-86ea-ee7d9501c555","metadata":{},"outputs":[],"source":["def generate_incorrect_data(pd: pd.DataFrame, columns):\n    \"\"\"Return modified dataframe with incorrect data.\n    \"\"\"\n    for column in columns:\n        pd[column] = pd[column].where(lambda x: np.random.rand(len(df)) \u003e 0.07, np.nan)\n        pd[column] = pd[column].where(lambda x: np.random.rand(len(df)) \u003e 0.07, np.random.uniform(-1.0, 0.0))\n        pd[column] = pd[column].where(lambda x: np.random.rand(len(df)) \u003e 0.07, \n                                      ''.join(random.choices(string.ascii_letters, k=7)))\n    return pd"]},{"cell_type":"markdown","id":"f17a920b-a18a-4287-8725-11cc1153de4c","metadata":{},"outputs":[],"source":["You probably have noticed strange keyword \u003ccode\u003elambda\u003c/code\u003e in our function.\n","\n","#### What is lambda?\n","\n","\u003cpre\u003e\u003ci\u003eLambda\u003c/i\u003e - an anonymous function which we can pass in instantly without defining a name like a full traditional function.\u003c/pre\u003e\n","\n","We can operate with a lambda function in relation to both the columns and rows of the Pandas dataframe. In our instance we apply lambda function to each cell of passed to function columns.\n"]},{"cell_type":"markdown","id":"6caf46ac-366e-4a23-840c-03b95561886f","metadata":{},"outputs":[],"source":["We should keep our initial dataframe unchangable to use in further steps of current lab. Thus, the following dataframe manipulation will be produced on the copied dataframe. To make a copy of this object’s indices and data use \u003ccode\u003ecopy()\u003c/code\u003e function:\n"]},{"cell_type":"raw","id":"2ec9efae-80a2-45b4-938b-722321480d2c","metadata":{},"outputs":[],"source":["dataframe.copy()"]},{"cell_type":"code","id":"33efd155-f034-4515-b7ec-e165081fa178","metadata":{},"outputs":[],"source":["df_missing = df.copy()\n\ncolumns = ['Open', 'High', 'Low', 'Close']\ndf_missing = generate_incorrect_data(df_missing, columns)\ndf_missing"]},{"cell_type":"markdown","id":"5e3ade7b-aa47-4a21-8730-4883ea29ebe5","metadata":{},"outputs":[],"source":["As we can see, incorrect data appeared in the dataframe; those may hinder our further analysis.\n","\n","So, how do we identify all those missing and incorrect values and deal with them?\n","\n","\u003cstrong\u003eHow to work with missing data?\u003c/strong\u003e\n","\n","Steps for working with missing data:\n","\u003col\u003e\n","    \u003cli\u003e\u003cem\u003eIdentify missing data\u003c/em\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003cem\u003eDeal with missing data\u003c/em\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003cem\u003eCorrect data format\u003c/em\u003e\u003c/li\u003e\n","\u003c/ol\u003e\n"]},{"cell_type":"markdown","id":"6c877f3c-d96a-49dc-921a-bd7a7cd2221a","metadata":{},"outputs":[],"source":["## 3. Identify and handle missing values\n","\n","### 3.1 Identify missing values\n","\n","#### Convert incorrect data to NaN\n","\n","We replace incorrect data with NaN (Not a Number), Python's default missing value marker for reasons of computational speed and convenience. Here we use the function: \n"," \u003cpre\u003e.replace(A, B, inplace = True) \u003c/pre\u003e\n","\n","to replace A by B.\n","\u003cbr\u003e\n","\u003cp\u003eTry to replace string cells in dataframe using this function. Since when generating incorrect data we randomized strings we should use \u003cstrong\u003eregular expressions\u003c/strong\u003e (\u003cem\u003eaka Regex\u003c/em\u003e) to determine all dataframe values that correspond to a sequence of uppercase and lowercase letters.\n","\u003cpre\u003e\u003ci\u003eRegEx\u003c/i\u003e, or regular expression - a sequence of characters that forms a search pattern.\u003c/pre\u003e\n","\u003c/p\u003e\n","\u003cp\u003e\n","To accomplish this we need to specify the parameter \u003ccode\u003eregex=True\u003c/code\u003e in \u003ccode\u003ereplace()\u003c/code\u003e method to use regular expressions and pass the regex itself. To determine the strings of letters use regex \u003ccode\u003er'^[A-Za-z]+$'\u003c/code\u003e. \n","\u003c/p\u003e\n"]},{"cell_type":"code","id":"49868254-5cec-4f13-aacb-637b2b89f4bc","metadata":{},"outputs":[],"source":["# replace strings with NaN\ndf_missing.replace(r'^[A-Za-z]+$', np.nan, inplace=True, regex=True)\ndf_missing"]},{"cell_type":"markdown","id":"b03a2a9e-92bd-4019-ab70-10d24aff1fe9","metadata":{},"outputs":[],"source":["The next step we need to replace the negative values ​​in the dataframe with \u003ccode\u003eNaN\u003c/code\u003e. To accomplish this we use numpy function \u003ccode\u003ewhere()\u003c/code\u003e specifying a condition \u003ccode\u003ex \u003e 0\u003c/code\u003e. If condition equals True, then yield \u003ccode\u003ex\u003c/code\u003e, otherwise yield \u003ccode\u003eNaN\u003c/code\u003e.\n"]},{"cell_type":"code","id":"0c07c38b-1e75-4e15-be48-00472b548630","metadata":{},"outputs":[],"source":["# replace negative values with NaN\nfor column in columns:\n    df_missing[column] = df_missing[column].where(lambda x: x \u003e 0, np.nan)\ndf_missing"]},{"cell_type":"markdown","id":"ffc65bb9-8f11-4b92-beea-c2fbd39cc1bd","metadata":{},"outputs":[],"source":["#### Evaluating for Missing Data\n","\n","The missing values are converted by default. We use the following functions to identify these missing values. There are two methods to detect missing data:\n","\n","\u003col\u003e\n","    \u003cli\u003e\u003ccode\u003e.isnull()\u003c/code\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ccode\u003e.notnull()\u003ccode\u003e\u003c/li\u003e\n","\u003c/ol\u003e\n","\n","The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.\n"]},{"cell_type":"code","id":"7397c058-73b2-4af9-9ffe-6b00e035172e","metadata":{},"outputs":[],"source":["missing_data = df_missing.isnull()\nmissing_data.head()"]},{"cell_type":"markdown","id":"06e6ab2b-d828-473c-950a-4f071bd539f0","metadata":{},"outputs":[],"source":["\u003cstrong\u003eTrue\u003c/strong\u003e means the value is a missing value, whereas \u003cstrong\u003eFalse\u003c/strong\u003e means the value is not a missing value.\n"]},{"cell_type":"markdown","id":"419db848-fbae-410a-8e60-a80705e760ef","metadata":{},"outputs":[],"source":["#### Count missing values in each column\n","\u003cp\u003e\n","Using a for loop in Python, we can quickly figure out the number of missing values in each column. \n","\u003c/p\u003e\n","\n","As mentioned above, \u003cstrong\u003eTrue\u003c/strong\u003e represents a missing value and \u003cstrong\u003eFalse\u003c/strong\u003e means the value is present in the dataset.  In the body of the for loop the method \u003ccode\u003e.value_counts()\u003c/code\u003e counts the number of \u003cstrong\u003eTrue\u003c/strong\u003e values. \n","\n"]},{"cell_type":"code","id":"611391db-096d-403f-af9d-4af24e85b017","metadata":{},"outputs":[],"source":["for column in missing_data.columns.values.tolist():\n    print(column)\n    print(missing_data[column].value_counts())\n    print('')"]},{"cell_type":"markdown","id":"d36f15ad-96fc-4bfc-8614-d8d15f5c3ca4","metadata":{},"outputs":[],"source":["Based on the summary above, three of the columns containing missing data.\n"]},{"cell_type":"markdown","id":"cc96920b-16c0-4416-bf8f-cbbff36d3482","metadata":{},"outputs":[],"source":["### 3.2 Deal with missing data\n","\n","#### How to deal with missing data?\n","\u003col\u003e\n","    \u003cli\u003e\u003cstrong\u003eDrop data\u003c/strong\u003e\u003cbr\u003e\n","        a. Drop \u003cem\u003ethe whole row\u003c/em\u003e\u003cbr\u003e\n","        b. Drop \u003cem\u003ethe whole column\u003c/em\u003e\n","    \u003c/li\u003e\n","    \u003cli\u003e\u003cstrong\u003eReplace data\u003c/strong\u003e\u003cbr\u003e\n","        a. Replace it \u003cem\u003eby mean\u003c/em\u003e\u003cbr\u003e\n","        b. Replace it \u003cem\u003eby frequency\u003c/em\u003e\u003cbr\u003e\n","        c. Replace it \u003cem\u003ebased on other functions\u003c/em\u003e\n","    \u003c/li\u003e\n","\u003c/ol\u003e\n"]},{"cell_type":"markdown","id":"b2283a87-aae7-4440-bd1a-2b3122084672","metadata":{},"outputs":[],"source":["Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely.\n","We have some freedom in choosing which method to replace data. However, some methods may seem more reasonable than others. \n","\n","We will apply different methods such as replacing by \u003cstrong\u003einterpolation\u003c/strong\u003e with diverse techniques. Then we will compare each of replacing ways by calculating precision between restored dataframe and the real one. \n","\n","#### What is an interpolation?\n","\n","In the mathematical field of numerical analysis, \u003cstrong\u003e\u003cem\u003einterpolation\u003c/em\u003e\u003c/strong\u003e is a type of estimation, a method of constructing (finding) new data points based on the range of a discrete set of known data points.\n"]},{"cell_type":"markdown","id":"d063bd36-84f9-4a2d-b6a2-72b38224724c","metadata":{},"outputs":[],"source":["For further comparison of the obtained dataframes with the initial one, we will create a separate dataframe to record the data recovery method and calculated accuracy.\n"]},{"cell_type":"code","id":"1b83c192-b0f5-493e-a0ac-fdfb82300d6f","metadata":{},"outputs":[],"source":["df_precision = pd.DataFrame({\"method\":[], \"MSE\": [], \"MAPE\": []})"]},{"cell_type":"markdown","id":"86aeb45a-2c22-4295-9439-34f27278d78c","metadata":{},"outputs":[],"source":["#### Estimation metrics\n"]},{"cell_type":"markdown","id":"a1ab4b22-e524-48e2-aca7-6782d52f75ab","metadata":{},"outputs":[],"source":["For calculating difference between restored and initial dataframes we will use $Mean\\ Squared\\ Error$ and $Mean\\ Absolute\\ Percentage$ $Error\\$. \n","\n","#### What is Mean Squared Error (MSE)?\n","\n","The $Mean\\ Squared\\ Error$ is a measure of the quality of an estimator. As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.\n","\n","$$\n","MSE = \\frac{1}{n} \\sum \\limits _{i=1} ^{n} (A_i - F_i)^{2},\n","$$ \n","\u003ccenter\u003ewhere $A_i$ — the actual value, $F_i$ — the forecast value.\u003c/center\u003e\n","\n","Read more about it \u003ca href=\"https://en.wikipedia.org/wiki/Mean_squared_error?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08LZES2357-2023-01-01\"\u003ehere\u003c/a\u003e.\n","\n","\u003cp\u003e\n","To calculate it we will use \u003ccode\u003emean_squared_error(y_true, y_pred)\u003c/code\u003e from \u003cstrong\u003e\u003cem\u003eskicit-learn library\u003c/em\u003e\u003c/strong\u003e, where \u003ccode\u003ey_true\u003c/code\u003e represents \u003cem\u003eground truth (correct) target values\u003c/em\u003e and \u003ccode\u003ey_pred\u003c/code\u003e corresponds to \u003cem\u003eestimated target values\u003c/em\u003e.\n","\u003c/p\u003e\n","\n","#### What is Mean Absolute Percentage Error (MAPE)?\n","\n","A statistic known as $Mean\\ Absolute\\ Percentage\\ Error$ is used to assess how accurate a forecasting technique is. It represents the average of the absolute percentage errors of each entry in a dataset to calculate how accurate the forecasted quantities were in comparison with the actual quantities. \n","\n","$$\n","MAPE = \\frac{100}{n} \\sum \\limits _{i=1} ^{n} \\left\\lvert \\frac{A_i - F_i}{A_i} \\right\\rvert,\n","$$ \n","\u003ccenter\u003ewhere $A_i$ — the actual value, $F_i$ — the forecast value.\u003ccenter\u003e\n","\n","\u003cbr\u003e\n","\n","The absolute value of this ratio is summed for every forecast point in time and divided by number of fitted pooints $n$. Read more about it \u003ca href=\"https://en.wikipedia.org/wiki/Mean_absolute_percentage_error?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08LZES2357-2023-01-01\"\u003ehere\u003c/a\u003e.\n","\u003cbr\u003e\n","\n","To calculate MAPE we can use \u003ccode\u003emean_absolute_percentage_error(y_true, y_pred)\u003c/code\u003e from \u003cstrong\u003e\u003cem\u003eskicit-learn library\u003c/em\u003e\u003c/strong\u003e where \u003ccode\u003ey_true\u003c/code\u003e represents ground truth (correct) target values and \u003ccode\u003ey_pred\u003c/code\u003e corresponds to estimated target values.\n","\n","However, let's use scikit-learn \u003ccode\u003emean_squared_error()\u003c/code\u003e function for calculating $MSE$ and create a custom function to calculate $MAPE$.  \n"]},{"cell_type":"code","id":"29a09a3a-7183-4bbf-8391-c289a07c5f5d","metadata":{},"outputs":[],"source":["def mape(y_true, y_pred):\n    \"\"\"Return Mean Absolute Percentage Error (MAPE).\n    \"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"]},{"cell_type":"markdown","id":"0550c50c-0a13-48ab-a2d5-31e9b9101fa6","metadata":{},"outputs":[],"source":["Excellent! Let's move to replacing missing data.\n"]},{"cell_type":"markdown","id":"0b30192b-b57b-4a3f-9e57-003db56d03a7","metadata":{},"outputs":[],"source":["#### Replace \"NaN\" with the interpolation techniques\n"]},{"cell_type":"markdown","id":"2ecbcf36-4f97-46d5-b0d8-367a11f2165e","metadata":{},"outputs":[],"source":["Let us use \u003ccode\u003einterpolate(method, inplace=True)\u003c/code\u003e to fill \u003ccode\u003eNaN\u003c/code\u003e values using an interpolation, where parameter \u003ccode\u003emethod\u003c/code\u003e specifies interpolation technique to use, and \u003ccode\u003einplace=True\u003c/code\u003e - modify and update the current dataframe.\n"]},{"cell_type":"raw","id":"a2850359-ddd1-4b90-b142-49690f140465","metadata":{},"outputs":[],"source":["dataframe.interpolate(method, inplace=True)"]},{"cell_type":"markdown","id":"0696c8d2-bbfc-4a2b-9c29-00e62e926ba7","metadata":{},"outputs":[],"source":["Let's try to remove missing values with \u003cem\u003elinear interpolation technique\u003c/em\u003e. \n"]},{"cell_type":"code","id":"5c9fa3cc-bc59-4d5e-a1dc-6f55e837c189","metadata":{},"outputs":[],"source":["# fill NaN values using an interpolation method \ndf_pred = df_missing.astype('float').interpolate(method='linear')\ndf_pred"]},{"cell_type":"markdown","id":"b8784a3f-c0e6-45ab-bea9-f8678e5a47d6","metadata":{},"outputs":[],"source":["As we can see, missing data disappeared.\n","\n","Now when we've tested how interpolation works, let's test its other methods and compare their accuracy.\n"]},{"cell_type":"markdown","id":"bb2a1e69-2753-4333-9164-1513a2063a28","metadata":{},"outputs":[],"source":["\u003cp\u003e\n","Note that even after using interpolation the first and last entries in the columns may remain \u003ccode\u003eNaN\u003c/code\u003e (if incorrect data is generated in such positions), because there is no data before it to use for interpolation. To fix this issue we use \u003ccode\u003efillna(method, inplace)\u003c/code\u003e with \u003ccode\u003e'ffill'\u003c/code\u003e and \u003ccode\u003e'bfill'\u003c/code\u003e methods to use previous and next valid observation to fill gaps.\n","\u003c/p\u003e\n","\u003cp\u003e\n","Along with that, we need to keep updating our dataframe \u003ccode\u003edf_precision\u003c/code\u003e responsible of storing Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE) values of each data restoring method used.\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"66299fee-3c57-4aff-97d7-378ac0c68ea6","metadata":{},"outputs":[],"source":["Pay attention that both \u003cem\u003epolynomial\u003c/em\u003e and \u003cem\u003espline\u003c/em\u003e techniques require specifying an \u003ccode\u003eorder\u003c/code\u003e parameter, e.g. \u003ccode\u003edf.interpolate(method='polynomial', order=5)\u003c/code\u003e.\n","\n","Considering the amount of data and interpolation methods, the following code may take some time to execute.\n"]},{"cell_type":"code","id":"9a38fad7-7039-470c-86c4-d2bb12742b9b","metadata":{},"outputs":[],"source":["methods = [\"linear\", \"nearest\", \"quadratic\", \"cubic\"]\norder_methods = [\"spline\", \"polynomial\"]\n\nfor method in methods:\n    # fill NaN values using an interpolation method \n    df_pred = df_missing.astype('float').interpolate(method=method)\n    df_pred.fillna(method=\"ffill\", inplace=True)\n    df_pred.fillna(method='bfill', inplace=True)\n    \n    # calculate MSE and MAPE\n    df_precision.loc[len(df_precision.index)] = [method, mean_squared_error(df[columns], df_pred[columns]), mape(df[columns], df_pred[columns])]\n\nfor method in order_methods:\n    # fill NaN values using an interpolation method\n    df_pred = df_missing.astype('float').interpolate(method=method, order=2)\n    df_pred.fillna(method=\"ffill\", inplace=True)\n    df_pred.fillna(method='bfill', inplace=True)\n    \n    # calculate MSE and MAPE\n    df_precision.loc[len(df_precision.index)] = [method, mean_squared_error(df[columns], df_pred[columns]), mape(df[columns], df_pred[columns])]\n\ndf_precision.set_index('method', inplace=True)\ndf_precision"]},{"cell_type":"markdown","id":"1c3824f3-1d87-4f1f-9005-913b3eb3daed","metadata":{},"outputs":[],"source":["As we can see, \u003cem\u003ethe linear interpolation method\u003c/em\u003e did the best job in restoring data in our dataframe. The MSE and MAPE value it produced are the smallest among the others.\n"]},{"cell_type":"markdown","id":"762e5076-f1d1-4534-a454-97f0073af641","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e Question  #1: \u003c/h1\u003e\n","\n","\u003cb\u003eBased on the example above, use before declared function to generate incorrect data in \"Close\" column, replace it with \u003ccode\u003eNaN\u003c/code\u003e values. Also fill missing data by using \u003cem\u003elinear interpolation method\u003c/em\u003e.\u003c/b\u003e\n","\n","\u003c/div\u003e\n"]},{"cell_type":"markdown","id":"b6b317bb-7c32-4bee-b039-5b839a1021c5","metadata":{},"outputs":[],"source":["\u003cstrong\u003e\u003cem\u003eNote:\u003c/em\u003e\u003c/strong\u003e to keep our initial dataframe unchangable make a copy of it using \u003ccode\u003edf.copy()\u003c/code\u003e method.\n"]},{"cell_type":"code","id":"ab23ffd7-bfc5-41da-88ec-009ac8ee8023","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n\n# copy initial dataframe\ndf_copied = df.copy()\n\n# generate incorrect data in the \"Close\" column\ndf_copied = generate_incorrect_data(df_copied, [\"Close\"])\n\n# replace strings in \"Close\" column with NaN value\ndf_copied.replace(r'^[A-Za-z]+$', np.nan, inplace=True, regex=True)\n\n# replace negative values in \"Close\" column with NaN value\ndf_copied[\"Close\"] = df[\"Close\"].where(lambda x: x \u003e 0, np.nan)\n\n# replace NaN by linear interpolation\ndf_copied[\"Close\"].interpolate(method=\"linear\", inplace=True)\n\n# check changes in \"Close\" column\ndf_copied[[\"Close\"]].head()"]},{"cell_type":"markdown","id":"43cb3b45-8baf-46a0-b585-57979b9b8308","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# copy initial dataframe\n","df_copied = df.copy()\n","\n","# generate incorrect data in the \"Close\" column\n","df_copied = generate_incorrect_data(df_copied, [\"Close\"])\n","\n","# replace strings in \"Close\" column with NaN value\n","df_copied.replace(r'^[A-Za-z]+$', np.nan, inplace=True, regex=True)\n","\n","# replace negative values in \"Close\" column with NaN value\n","df_copied[\"Close\"] = df[\"Close\"].where(lambda x: x \u003e 0, np.nan)\n","\n","# replace NaN by linear interpolation\n","df_copied[\"Close\"].interpolate(method=\"linear\", inplace=True)\n","\n","# check changes in \"Close\" column\n","df_copied[[\"Close\"]].head()\n","```\n","    \n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"68fdd066-63d9-4e21-b5f3-13120ce2c316","metadata":{},"outputs":[],"source":["\u003cb\u003eGood!\u003c/b\u003e Now, we have a dataset with no missing values.\n"]},{"cell_type":"markdown","id":"492da7b0-38d0-42cd-bf6c-9b46b51d1e20","metadata":{},"outputs":[],"source":["### 3.3 Correct data format\n","\n","\u003cp\u003eThe last step in data cleaning is checking and making sure that all data is in the correct format (\u003cstrong\u003eint, float, text\u003c/strong\u003e or other).\u003c/p\u003e\n","\n","In Pandas, we use:\n","\u003cul\u003e\n","    \u003cli\u003e\u003ccode\u003e.dtype()\u003c/code\u003e to check the data type\u003c/li\u003e\n","    \u003cli\u003e\u003ccode\u003e.astype()\u003c/code\u003e to change the data type\u003c/li\u003e\n","\u003c/ul\u003e\n"]},{"cell_type":"markdown","id":"8aa0345f-1ada-4804-a994-60d5147113b1","metadata":{},"outputs":[],"source":["Let's list the data types for each column:\n"]},{"cell_type":"code","id":"090c8d0d-75c7-4e39-8baa-e285c2ffb74e","metadata":{},"outputs":[],"source":["df.dtypes"]},{"cell_type":"markdown","id":"d296099b-5d51-4472-b733-f73557a03910","metadata":{},"outputs":[],"source":["\u003cp\u003eAs we can see above, all columns are of the correct data type. Numerical variables should have type \u003ccode\u003e'float'\u003c/code\u003e or \u003ccode\u003e'int'\u003c/code\u003e, and variables with strings such as categories should have type \u003ccode\u003e'object'\u003c/code\u003e. \n","\u003cbr\u003e\n","For example, 'Open' and 'Count' variables are numerical values, so we should expect them to be of the type \u003ccode\u003e'float'\u003c/code\u003e or \u003ccode\u003e'int'\u003c/code\u003e.\n","\u003c/p\u003e\n","\n","If we have to convert data types into a proper format for each column, we use the \u003ccode\u003eastype()\u003c/code\u003e method:\n"]},{"cell_type":"raw","id":"92177a7d-1124-4cde-9529-9b67f03ab9ee","metadata":{},"outputs":[],"source":["df['column_name'] = df['column_name'].astype('proper_type')"]},{"cell_type":"markdown","id":"e84d6011-9a51-493c-ae96-696dcfe7d78a","metadata":{},"outputs":[],"source":["\u003cb\u003eWonderful!\u003c/b\u003e\n","\n","Now we have finally obtained the cleaned dataset with no missing values with all data in its proper format.\n"]},{"cell_type":"markdown","id":"48704603-08a0-4bea-afd2-b75e81e9dec2","metadata":{},"outputs":[],"source":["## 4. Data Standardization\n","\u003cp\u003e\n","Data is usually collected from different agencies in different formats.\n","Data standardization is also a term for a particular type of data normalization, where we subtract the mean and divide by the standard deviation.\n","\u003c/p\u003e\n","\n","#### What is data standardization?\n","\n","\u003cp\u003eStandardization is the process of transforming data into a common format, allowing the researcher to make the meaningful comparison.\n","\u003c/p\u003e\n","\n","\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e \u003cem\u003etransform BUSD to USDT\u003c/em\u003e\n","\u003cp\u003eIn our dataset, the columns \"Open\", \"High\", \"Low\", \"Close\", \"Volume\" and \"Price\" are represented by BUSD (Binance USD) unit. However, in most cases USDT is commonly used. We will need to apply \u003cstrong\u003edata transformation\u003c/strong\u003e to transform BUSD into USDT.\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"9873cbdf-0b8b-439e-95d5-bec638b8c940","metadata":{},"outputs":[],"source":["\u003cp\u003eLet us start by solving the issue with obtaining current exchange rate. We will use \u003ccode\u003epyfetch()\u003c/code\u003emethod to make HTTP requests to official Binance API and fetch exchange rate data from it. \u003c/p\u003e\n"]},{"cell_type":"code","id":"5c38e061-d7b1-4c07-80d6-f29b5e2fe03c","metadata":{},"outputs":[],"source":["# get updated USDT rate\nresponse = requests.get('https://api.binance.com/sapi/v1/convert/exchangeInfo?fromAsset=BUSD\u0026toAsset=USDT')\nresponse = json.loads(response.text)"]},{"cell_type":"markdown","id":"5cc127b7-0508-4f0d-9cdc-17918a7841cb","metadata":{},"outputs":[],"source":["Returned value is in \u003cstrong\u003eJSON format\u003cstrong\u003e.\n","\n","\u003cpre\u003e\u003cem\u003e\u003cstrong\u003eJavaScript Object Notation (JSON)\u003c/strong\u003e\u003c/em\u003e is a standard text-based format for representing structured data based on JavaScript object syntax. It is frequently employed for data transmission in online applications (e.g., sending some data from the server to the client, so it can be displayed on a web page, or vice versa).\u003c/pre\u003e\n","\n","Then we should check the HTTP status response. \u003cstrong\u003e200 (OK success)\u003c/strong\u003e code indicates that the request has succeeded. For obtaining current rate we need to access \u003cem\u003e\"toAssetMinAmount\"\u003c/em\u003e field in our response.\n"]},{"cell_type":"code","id":"22ef5b4a-184c-469f-8573-d4850abc0546","metadata":{},"outputs":[],"source":["# if the API is unavailable we set fixed rate\ntry:\n    if response.status != 200:\n        rate = 0.999707\n    else:\n        rate = float(response[0][\"toAssetMinAmount\"])\nexcept:\n     rate = 0.999707\n\nprint(f\"The exchange rate is 1 BUSD = {rate} USDT\")"]},{"cell_type":"markdown","id":"1e58c1b6-762c-4e3b-af31-df8144f6782d","metadata":{},"outputs":[],"source":["\u003cp\u003e\n","The next step we calculate a new value for needed currency of each following columns: \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Price\".\n","\u003c/p\u003e\n"]},{"cell_type":"code","id":"8b60b590-d3bb-4ed0-89bb-f48b2557cd23","metadata":{},"outputs":[],"source":["columns_to_convert = ['Open', 'High', 'Low', 'Close', 'Volume', 'Price']\nfor column in columns_to_convert:\n    df[f'{column}_USDT'] = df[column] * rate"]},{"cell_type":"markdown","id":"8403a767-eb61-48cf-94fe-f5541a50ab2f","metadata":{},"outputs":[],"source":["\u003cp\u003eDo not forget to drop unnecessary columns in BUSD currency.\n","\u003c/p\u003e\n"]},{"cell_type":"code","id":"26912abf-b189-4592-a7b8-c0a8d243968c","metadata":{},"outputs":[],"source":["# drop unnecessary columns\ndf.drop(columns_to_convert, axis=1, inplace=True)"]},{"cell_type":"markdown","id":"7e3471cc-d0c3-4bd7-abc9-cdcb7a13f57d","metadata":{},"outputs":[],"source":["Finally, we check our transformed data.\n"]},{"cell_type":"code","id":"4c3c01e0-d8d9-417f-8b28-249aaf63e9f7","metadata":{},"outputs":[],"source":["# check your transformed data\ndf[['Open_USDT', 'High_USDT', 'Low_USDT', 'Close_USDT', 'Volume_USDT', 'Price_USDT']].head()"]},{"cell_type":"markdown","id":"25ccf3ca-d5ee-4fd0-b6f4-d1553f49ef1b","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e Question  #2: \u003c/h1\u003e\n","\n","\u003cb\u003eAccording to the example above, transform \"Price_USDT\" column vales from USDT currency to EUR and change the name of column to \"Price_EUR\".\u003c/b\u003e\n","\u003c/div\u003e\n"]},{"cell_type":"markdown","id":"5d7b33b9-e251-44f2-897e-a40ed5c52acc","metadata":{},"outputs":[],"source":["\u003cstrong\u003e\u003cem\u003eNote:\u003c/em\u003e\u003c/strong\u003e. to receive the current data on the euro exchange rate, use the following URL: https://api.binance.com/sapi/v1/convert/exchangeInfo?fromAsset=USDT\u0026toAsset=EUR.\n"]},{"cell_type":"code","id":"20555a8e-49a0-4d30-a9cc-b9c26ef849f4","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n\n# get current EUR rate\nresponse = requests.get('https://api.binance.com/sapi/v1/convert/exchangeInfo?fromAsset=USDT\u0026toAsset=EUR')\nresponse = json.loads(response.text)\n\n# if the API is unavailable we set fixed rate\ntry:\n    if response.status != 200:\n        rate = 0.92\n    else:\n        rate = float(response[0]['toAssetMinAmount'])\nexcept:\n    rate = 0.92\n    \nprint(f'The exchange rate is 1 USDT = {rate} EUR')\n\n# change rate in the column\ndf['Price_USDT'] = df['Price_USDT'] * rate\n\n# rename column name from \"Price_USDT\" to \"Price_EUR\"\ndf.rename(columns={'Price_USDT':'Price_EUR'}, inplace=True)\n\n# check your transformed data\ndf[['Price_EUR']].head()"]},{"cell_type":"markdown","id":"38943db3-b51b-4db1-b23e-5680546607ba","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# get current EUR rate\n","response = requests.get('https://api.binance.com/sapi/v1/convert/exchangeInfo?fromAsset=USDT\u0026toAsset=EUR')\n","response = json.loads(response.text)\n","\n","# if the API is unavailable we set fixed rate\n","try:\n","    if response.status != 200:\n","        rate = 0.92\n","    else:\n","        rate = float(response[0]['toAssetMinAmount'])\n","except:\n","    rate = 0.92\n","    \n","print(f'The exchange rate is 1 USDT = {rate} EUR')\n","\n","# change rate in the column\n","df['Price_USDT'] = df['Price_USDT'] * rate\n","\n","# rename column name from \"Price_USDT\" to \"Price_EUR\"\n","df.rename(columns={'Price_USDT':'Price_EUR'}, inplace=True)\n","\n","# check your transformed data\n","df[['Price_EUR']].head()\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"bb6a75d1-089d-4b95-a3b3-59acd06e08e9","metadata":{},"outputs":[],"source":["## 5. Data Normalization\n","\n","#### Why normalization?\n","\n","\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNormalization\u003c/em\u003e\u003c/strong\u003e is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable so the variable average is 0, scaling the variable so the variance is 1, or scaling the variable so the variable values range from 0 to 1.\n","\u003c/p\u003e\n","\n","\u003cstrong\u003eExample:\u003c/strong\u003e\n","\n","\u003cp\u003eTo demonstrate normalization, let's say we want to scale the \"Open_USDT\" column.\u003c/p\u003e\n","\u003cp\u003e\u003cstrong\u003eTarget:\u003c/strong\u003e would like to normalize this variable so its value ranges from 0 to 1.\u003c/p\u003e\n","\n","\u003cp\u003e\u003cstrong\u003eApproach:\u003c/strong\u003e replace original value by:\n","\u003cli\u003e formula $ \\frac{original\\ value}{maximum\\ value}$ \u003c/li\u003e\n","\u003cli\u003e skicit-learn estimator $ MinMaxScaler $.\u003c/li\u003e\n","\u003c/p\u003e\n","\u003cp\u003e\n","The $ MinMaxScaler $ transformation is given by:\n","\n","$$\n","X_{std} = \\frac{ X - X_{min}}{ X_{max} - X_{min}}, \\\\\\\\\\\\\n","X_{scaled} = X_{std}\\times (max - min) + min,\n","$$\n","        \n","\u003ccenter\u003ewhere $ min, \\ max$ are upper and lower borders in scaling range.\u003c/center\u003e\n","\u003c/p\u003e\n","\n","\u003cp\u003eLet's scale the \"Open_USDT\" column by first formula.\u003c/p\u003e\n"]},{"cell_type":"code","id":"5369deb9-2c9e-4137-9fa9-cccca5642878","metadata":{},"outputs":[],"source":["# replace original value by (original value)/(maximum value)\ndf['Open_USDT'] = df['Open_USDT']/df['Open_USDT'].max()\ndf[['Open_USDT']].head()"]},{"cell_type":"markdown","id":"67d3498f-eeb8-4b4c-97b0-be8f9366a269","metadata":{},"outputs":[],"source":["Let us do the same for \"Close_USDT\" column with \u003ccode\u003eMinMaxScaler()\u003c/code\u003e estimator using its \u003ccode\u003efit_transform()\u003c/code\u003e function. \n"]},{"cell_type":"code","id":"40f793e4-e865-453e-83a7-aa5b9627581c","metadata":{},"outputs":[],"source":["# replace (original value) by estimator MinMaxScaler\nscaler = MinMaxScaler()\ndf[['Close_USDT']] = scaler.fit_transform(df[['Close_USDT']])\ndf[['Close_USDT']].head()"]},{"cell_type":"markdown","id":"28142470-6151-41bc-9c34-daf18ef06257","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e Question #3: \u003c/h1\u003e\n","\n","\u003cb\u003eAccording to the example above, normalize the columns \"High_USDT\"  using formula $ \\frac{original\\ value}{maximum\\ value}$ and \"Low_USDT\" using $MinMaxScaler$ estimator.\u003c/b\u003e\n","\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"6ef1a552-1596-4315-832f-73ade4286da8","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n\n# normalize by (original value)/(maximum value)\ndf['High_USDT'] = df['High_USDT']/df['High_USDT'].max() \n\n#  normalize by MinMaxScaler estimator\ndf[['Low_USDT']] = scaler.fit_transform(df[['Low_USDT']])\n\n# show the scaled columns\ndf[['High_USDT', 'Low_USDT']].head()"]},{"cell_type":"markdown","id":"0960cb2c-0ed9-43e0-abcd-e994e9a66f62","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# normalize by (original value)/(maximum value)\n","df['High_USDT'] = df['High_USDT']/df['High_USDT'].max() \n","\n","#  normalize by MinMaxScaler estimator\n","df[['Low_USDT']] = scaler.fit_transform(df[['Low_USDT']])\n","\n","# show the scaled columns\n","df[['High_USDT', 'Low_USDT']].head()\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"e673f871-9561-422d-8b69-000d1c8754ca","metadata":{},"outputs":[],"source":["Here we can see we've normalized \"High_USDT\" and \"Low_USDT\" in the range of \\[0,1].\n"]},{"cell_type":"markdown","id":"30e590d0-be0a-4f0f-ac65-1b8a02453a50","metadata":{},"outputs":[],"source":["## 6. Binning\n","#### Why binning?\n","\u003cp\u003e\n","    \u003cstrong\u003e\u003cem\u003eBinning\u003c/em\u003e\u003c/strong\u003e is a process of transforming continuous numerical variables into discrete categorical 'bins' for grouped analysis.\n","\u003c/p\u003e\n","\n","\u003cstrong\u003eExample: \u003c/strong\u003e\n","\n","\u003cp\u003eIn our dataset, \"Volume\" is a real valued variable. What if we only care about the price difference with high price, medium price, and little price (3 types)? Can we rearrange them into three ‘bins' to simplify analysis? \u003c/p\u003e\n","\n","\u003cp\u003eWe will use the pandas method 'cut' to segment the 'Price_EUR' column into 3 bins.\u003c/p\u003e\n","\n","Firstly, let's convert data to correct format:\n"]},{"cell_type":"code","id":"b609ac51-a4b0-40c5-9b6b-3ad220ec00d9","metadata":{},"outputs":[],"source":["df['Price_EUR'] = df['Price_EUR'].astype(float, copy=True)"]},{"cell_type":"markdown","id":"8ef1ea9c-1840-428c-9958-d7159e3ba785","metadata":{},"outputs":[],"source":["Let's plot the histogram of volume to see what the distribution of volume looks like.\n"]},{"cell_type":"code","id":"1b0a7bf7-64eb-4af1-82fe-6763f2cd62cc","metadata":{},"outputs":[],"source":["# plot the distribution\nplt.pyplot.hist(df['Price_EUR'])\n\n# set x/y labels and plot title\nplt.pyplot.xlabel('Price_EUR, euro')\nplt.pyplot.ylabel('Count')\nplt.pyplot.title('Price_EUR bins')"]},{"cell_type":"markdown","id":"05c8f6f9-09e6-4c4f-a001-3840295889fa","metadata":{},"outputs":[],"source":["\u003cp\u003eWe would like 3 bins of equal size bandwidth so we use numpy's \u003ccode\u003elinspace(start_value, end_value, numbers_generated)\u003c/code\u003e function.\u003c/p\u003e\n","\u003cp\u003eSince we want to include the minimum value of price, we want to set \u003ccode\u003estart_value = min(df['Price_EUR'])\u003c/code\u003e.\u003c/p\u003e\n","\u003cp\u003eSince we want to include the maximum value of price, we want to set \u003ccode\u003eend_value = max(df['Price_EUR'])\u003c/code\u003e.\u003c/p\u003e\n","\u003cp\u003eSince we are building 3 bins of equal length, there should be 4 dividers, so \u003ccode\u003enumbers_generated = 4\u003c/code\u003e.\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"0a720f3c-f07c-4652-8802-bee69310aec9","metadata":{},"outputs":[],"source":["We build a bin array with a minimum value to a maximum value by using the bandwidth calculated above. The values will determine when one bin ends and another begins.\n"]},{"cell_type":"code","id":"e8e24d46-4da7-4e35-a820-d51b43300a99","metadata":{},"outputs":[],"source":["bins = np.linspace(min(df['Price_EUR']), max(df['Price_EUR']), 4)\nbins"]},{"cell_type":"markdown","id":"af8568ea-77aa-43f3-a10b-f0c9897492db","metadata":{},"outputs":[],"source":["We set group  names:\n"]},{"cell_type":"code","id":"c429a7d3-830a-4115-b4dd-a55450f817ad","metadata":{},"outputs":[],"source":["group_names = ['Price_Low', 'Price_Medium', 'Price_High']"]},{"cell_type":"markdown","id":"7d5e8db6-678f-4cba-bd8d-19d038a38293","metadata":{},"outputs":[],"source":["We apply the function \"cut\" to determine what each value of `df['Price_EUR']` belongs to.\n"]},{"cell_type":"code","id":"79fc8f71-9cd3-4843-8c1d-617f6c7f8c82","metadata":{},"outputs":[],"source":["df['Price-binned'] = pd.cut(df['Price_EUR'], bins, labels=group_names, include_lowest=True )\ndf[['Price_EUR', 'Price-binned']].head()"]},{"cell_type":"markdown","id":"0872c674-2ae1-453d-8bcd-b7cc70e4f6c3","metadata":{},"outputs":[],"source":["Let's see the number of vehicles in each bin:\n"]},{"cell_type":"code","id":"109673ed-2922-485b-bbd2-e83900e008c4","metadata":{},"outputs":[],"source":["df['Price-binned'].value_counts()"]},{"cell_type":"markdown","id":"d7a1b66c-8b0d-4b48-a672-0ff36410f979","metadata":{},"outputs":[],"source":["Let's plot the distribution of each bin:\n"]},{"cell_type":"code","id":"8a69756a-2361-4ac9-9745-ca7eed754afd","metadata":{},"outputs":[],"source":["# plot the distribution\npyplot.bar(group_names, df['Price-binned'].value_counts())\n\n# set x/y labels and plot title\nplt.pyplot.xlabel(\"Price\")\nplt.pyplot.ylabel(\"Count\")\nplt.pyplot.title(\"Price bins\")"]},{"cell_type":"markdown","id":"1b58fd48-c17c-4837-810d-62cbe4cf385f","metadata":{},"outputs":[],"source":["\u003cp\u003e\n","    Look at the dataframe above carefully. You will find that the last column provides the bins for \u003cstrong\u003e\"Price\"\u003c/strong\u003e based on 3 categories: \u003cstrong\u003e\"Price_Low\"\u003c/strong\u003e, \u003cstrong\u003e\"Price_Medium\"\u003c/strong\u003e and \u003cstrong\u003e\"Price_High\"\u003c/strong\u003e. \n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"b0d9d8ba-400d-441c-bd70-259d81f2db6c","metadata":{},"outputs":[],"source":["#### Bins Visualization\n","\n","Normally, a histogram is used to visualize the distribution of bins we created above. \n"]},{"cell_type":"code","id":"66a371fe-d27e-4a11-bca8-22edfbb9f49d","metadata":{},"outputs":[],"source":["# draw historgram of attribute \"Price_EUR\" with bins = 3\nplt.pyplot.hist(df[\"Price_EUR\"], bins=3)\n\n# set x/y labels and plot title\nplt.pyplot.xlabel(\"Price, euro\")\nplt.pyplot.ylabel(\"Count\")\nplt.pyplot.title(\"Price bins\")"]},{"cell_type":"markdown","id":"79666c01-959b-41e1-b600-21dc90bca46c","metadata":{},"outputs":[],"source":["The plot above shows the binning result for the attribute \"Price_EUR\".\n"]},{"cell_type":"markdown","id":"de01b2a8-25aa-458d-879a-4648f713477b","metadata":{},"outputs":[],"source":["## 7. Indicator Variable (or Dummy Variable)\n","#### What is an indicator variable?\n","\u003cp\u003e\n","    \u003cstrong\u003e\u003cem\u003eAn indicator variable (or dummy variable)\u003c/em\u003e\u003c/strong\u003e is a numerical variable used to label categories. They are called 'dummies' because the numbers themselves don't have inherent meaning. \n","\u003c/p\u003e\n","\n","#### Why we use indicator variables?\n","\n","\u003cp\u003e\n","    We use indicator variables so we can use categorical variables for regression analysis in the later modules.\n","\u003c/p\u003e\n","\n","\u003cstrong\u003eExample:\u003c/strong\u003e\n","\u003cp\u003e\n","    We see the column \u003ccode\u003e\"Price-binned\"\u003c/code\u003e has three unique values: \u003ccode\u003e\"Price_Low\"\u003c/code\u003e, \u003ccode\u003e\"Price_Medium\"\u003c/code\u003e or \u003ccode\u003e\"Price_High\"\u003c/code\u003e. Regression doesn't understand words, only numbers. To use this attribute in regression analysis, we convert \u003ccode\u003e\"Price-binned\"\u003c/code\u003e to indicator variables.\n","\u003c/p\u003e\n","\n","\u003cp\u003e\n","    We will use pandas method \u003ccode\u003eget_dummies()\u003c/code\u003e to assign numerical values to different categories of avg_price. \n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"a0ddb6f7-f712-4506-a6e0-45aea03c3f16","metadata":{},"outputs":[],"source":["Get the indicator variables and assign it to data frame \u003ccode\u003edummy_variable\\_1\u003c/code\u003e:\n"]},{"cell_type":"code","id":"b4f347e1-bc36-4f66-9812-f99dd5eeb0e4","metadata":{},"outputs":[],"source":["dummy_variable_1 = pd.get_dummies(df['Price-binned'])\ndummy_variable_1.head()"]},{"cell_type":"markdown","id":"fab6faa4-44d5-4373-857e-d7c3932d5e84","metadata":{},"outputs":[],"source":["In the dataframe, column \u003ccode\u003e'Price-binned'\u003c/code\u003e has values for \u003ccode\u003e'Price_Low'\u003c/code\u003e, \u003ccode\u003e'Price_Medium'\u003c/code\u003e and \u003ccode\u003e'Price_High'\u003c/code\u003e as 0s and 1s now.\n"]},{"cell_type":"code","id":"bd38e4ae-c8e0-442c-91e7-8c7737763302","metadata":{},"outputs":[],"source":["# merge data frame \"df\" and \"dummy_variable_1\"\ndf = pd.concat([df, dummy_variable_1], axis=1)\n\n# drop original column \"Price-binned\" from \"df\"\ndf.drop('Price-binned', axis=1, inplace=True)\ndf[['Price_EUR', 'Price_Low', 'Price_Medium', 'Price_High']].head()"]},{"cell_type":"markdown","id":"b5d18528-672b-4e53-937b-6660642e352a","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e Question  #4: \u003c/h1\u003e\n","\n","\u003cb\u003eSimilar to before, create bins for the column \"Open_USDT\" with three group names. Plot the distribution of each bin.\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"4c1aec7c-99dc-4bcf-ac86-7ad19459905b","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n\n# create bins\nbins = np.linspace(df['Open_USDT'].min(), df['Open_USDT'].max(), 4)\ngroup_names = ['Open_Low', 'Open_Medium', 'Open_High']\n\n# determine which bin each value of df['Open_USDT'] belongs to\ndf['Open_binned'] = pd.cut(df['Open_USDT'], bins, labels=group_names, include_lowest=True)\n\n# plot the distribution\npyplot.bar(group_names, df['Open_binned'].value_counts())\n\n# draw historgram of attribute \"Open_EUR\" with bins = 3\nplt.pyplot.hist(df['Open_USDT'])\n\n# set x/y labels and plot title\nplt.pyplot.xlabel('Open')\nplt.pyplot.ylabel('Count')\nplt.pyplot.title('Open bins')"]},{"cell_type":"markdown","id":"528033e6-2cf4-475b-a1f1-4be9d67f2269","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# create bins\n","bins = np.linspace(df['Open_USDT'].min(), df['Open_USDT'].max(), 4)\n","group_names = ['Open_Low', 'Open_Medium', 'Open_High']\n","\n","# determine which bin each value of df['Open_USDT'] belongs to\n","df['Open_binned'] = pd.cut(df['Open_USDT'], bins, labels=group_names, include_lowest=True)\n","\n","# plot the distribution\n","pyplot.bar(group_names, df['Open_binned'].value_counts())\n","\n","# draw historgram of attribute \"Open_EUR\" with bins = 3\n","plt.pyplot.hist(df['Open_USDT'])\n","\n","# set x/y labels and plot title\n","plt.pyplot.xlabel('Open')\n","plt.pyplot.ylabel('Count')\n","plt.pyplot.title('Open bins')\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"067fa353-0844-4b23-a70a-a73f02745913","metadata":{},"outputs":[],"source":["## 8. Resampling\n","#### What is a resampling?\n","\u003cp\u003e\n","    \u003cstrong\u003e\u003cem\u003eResampling\u003c/em\u003e\u003c/strong\u003e is a crucial method for time series analysis that enables you to freely choose the desired level of data resolution. You can either upsample, or increase the number of data points, such as by transforming 5-minute data into 1-minute data. \n","\u003c/p\u003e\n","\u003cp\u003e\n","    The basic syntax for resampling in Pandas is \u003ccode\u003edataframe.resample('desired resolution')\u003c/code\u003e method. Along with that, different aggregation function can be used.\n","\u003c/p\u003e\n","\u003cp\u003e\n","    Start by downsampling the series from 1 minute into 10-minute bins. Considering the semantics of our dataset, for the column \u003cstrong\u003e\"Open_USDT\"\u003c/strong\u003e we take \u003cem\u003ethe first value\u003c/em\u003e of a 10-minute interval, while for \u003cstrong\u003e\"Close_USDT\"\u003c/strong\u003e we have \u003cem\u003ethe last value\u003c/em\u003e. For \u003cstrong\u003e\"High_USDT\"\u003c/strong\u003e \u003cem\u003emaximum value\u003c/em\u003e within a 10-minute interval is taken, in accordance for \u003cstrong\u003e\"Low_USDT\"\u003c/strong\u003e we take \u003cem\u003eminimum value\u003c/em\u003e. Column \u003cstrong\u003e\"Volume_USDT\"\u003c/strong\u003e will store \u003cem\u003eall summed-up values\u003c/em\u003e within a 10-minute interval.\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"d86c5a6d-f95a-4eb3-806e-b0ccccc25ec3","metadata":{},"outputs":[],"source":["Firstly, let's copy the initial dataframe:\n"]},{"cell_type":"code","id":"f07d95a4-f4c2-431b-b23b-92adacb3b2fc","metadata":{},"outputs":[],"source":["df_resampled = df[['Open_USDT', 'High_USDT', 'Low_USDT', 'Close_USDT', 'Volume_USDT', 'Price_EUR']].copy()"]},{"cell_type":"markdown","id":"e688f008-7fc2-4e20-b759-bb22bd781832","metadata":{},"outputs":[],"source":["Now we are ready to perform resampling.\n"]},{"cell_type":"code","id":"769861fe-e545-4f11-a3b9-ca9a4addfb6f","metadata":{},"outputs":[],"source":["df_resampled.loc[:, 'Open_USDT':'Price_EUR'].resample(\"10min\").agg({\n    'Price_EUR': 'mean',\n    'Volume_USDT': 'sum',\n    'Open_USDT': 'first',\n    'High_USDT': 'max',\n    'Low_USDT': 'min',\n    'Close_USDT': 'last',\n}).head()"]},{"cell_type":"markdown","id":"7def5e6e-4a68-406c-a60f-d8247ca38a51","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e Question  #5 a): \u003c/h1\u003e\n","\n","\u003cb\u003eSimilar to before, downsample the \u003ccode\u003edf_resampled\u003c/code\u003e into 1-day bins.\u003c/b\u003e\n","\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"2ed794a7-9df4-4108-8155-a987c2e152a9","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n\ndf_resampled = df_resampled.loc[:, 'Open_USDT':'Price_EUR'].resample(\"1D\").agg({\n    'Price_EUR': 'mean',\n    'Volume_USDT': 'sum',\n    'Open_USDT': 'first',\n    'High_USDT': 'max',\n    'Low_USDT': 'min',\n    'Close_USDT': 'last',\n})\n\ndf_resampled.head()"]},{"cell_type":"markdown","id":"1f6ac1dd-ddde-4a2a-8b06-2fd6910349f1","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# downsample the series into 1-day bins.\n","df_resampled = df_resampled.loc[:, 'Open_USDT':'Price_EUR'].resample(\"1D\").agg({\n","    'Price_EUR': 'mean',\n","    'Volume_USDT': 'sum',\n","    'Open_USDT': 'first',\n","    'High_USDT': 'max',\n","    'Low_USDT': 'min',\n","    'Close_USDT': 'last',\n","})\n","\n","df_resampled.head()\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"b4cb9215-369d-4967-a1e4-9911134448ae","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e Question  #5 b): \u003c/h1\u003e\n","\n","\u003cb\u003eSimilar to before, create an indicator variable for the index column \"Time\" by days of week using \u003ccode\u003edf_resampled.index.day_name()\u003c/code\u003e.\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"2c36c1d1-6bb1-4e8a-b774-e5841de04471","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n\n# get indicator variables of day and assign it to dataframe \"dummy_variable_2\"\ndummy_variable_2 = pd.get_dummies(df_resampled.index.day_name())\ndummy_variable_2.set_index(df_resampled.index, inplace=True)\n\n# show first 5 instances of data frame \"dummy_variable_2\"\ndummy_variable_2.head()"]},{"cell_type":"markdown","id":"cb402aac-b27f-4a83-a7a6-2ca9c6e706f5","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# get indicator variables of day and assign it to dataframe \"dummy_variable_2\"\n","dummy_variable_2 = pd.get_dummies(df_resampled.index.day_name())\n","dummy_variable_2.set_index(df_resampled.index, inplace=True)\n","\n","# show first 5 instances of data frame \"dummy_variable_2\"\n","dummy_variable_2.head()\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"0d843fc9-4d08-4d9d-88f1-440c4e395298","metadata":{},"outputs":[],"source":["Great job! We have successfully reached the end.\n"]},{"cell_type":"markdown","id":"07eac8d2-7857-4e7c-a1ec-bfa4dcd677c5","metadata":{},"outputs":[],"source":["### Thank you for completing this lab!\n","\n","## Authors\n","\n","\u003ca href=\"https://author.skills.network/instructors/yaryna_beida?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08LZES2357-2023-01-01\"\u003eYaryna Beida\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08LZES2357-2023-01-01\"\u003eProf. Yaroslav Vyklyuk, DrSc, PhD\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/mariya_fleychuk?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08LZES2357-2023-01-01\"\u003eProf. Mariya Fleychuk, DrSc, PhD\u003c/a\u003e\n","\n","\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                  |\n","| ----------------- | ------- | ---------- | ----------------------------------- |\n","|     2023-03-04    |   1.0   |Yaryna Beida| Lab created                         |\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e © IBM Corporation 2023. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}