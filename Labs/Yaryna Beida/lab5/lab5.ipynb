{"cells":[{"cell_type":"markdown","id":"b4eb1121-4484-4043-a487-c104ab495f57","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"400\" alt=\"cognitiveclass.ai logo\"\u003e\n","\u003c/center\u003e\n","\n","# **Investigation relationships between exchange rate BTC/BUSD and ADOSC, NATR, TRANGE indicators**\n","\n","## Lab 5. Classification in finances\n","\n","Estimated time needed: **30** minutes\n","\n","\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \n","Для Марії\n","### The tasks:\n","*   \n","\n","\u003c/div\u003e\n","\n","\n","### Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Preprocess (normilize and transform categorical data and create DataSet\n","*   Features selection\n","*   Make classification\n","*   Visualize decision tree of classification model  \n","\n","### Table of Contents\n","\n","\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","\u003col\u003e\n","    \u003cli\u003eImport and Load Data\u003c/li\u003e\n","    \u003cli\u003eData preparation\u003c/li\u003e\n","        \u003cul\u003e\n","            \u003cli\u003eData transformation\u003c/li\u003e\n","            \u003cli\u003eEncoding and Normalization\u003c/li\u003e\n","        \u003c/ul\u003e\n","    \u003cli\u003eFeatures Selection\u003c/li\u003e\n","        \u003cul\u003e\n","            \u003cli\u003eChi-Squared Statistic\u003c/li\u003e\n","            \u003cli\u003eMutual Information Statistic\u003c/li\u003e\n","            \u003cli\u003eFeature Importance\u003c/li\u003e\n","        \u003c/ul\u003e  \n","    \u003cli\u003eClassification models\u003c/li\u003e\n","            \u003cul\u003e\n","                \u003cli\u003eTrain and Test DataSets Creation\u003c/li\u003e\n","                \u003cli\u003eExtra Trees Classifier\u003c/li\u003e\n","                \u003cli\u003eLogistic Regression\u003c/li\u003e\n","            \u003c/ul\u003e\n","    \u003cli\u003eDecision Tree\u003c/li\u003e\n","            \u003cul\u003e\n","                \u003cli\u003eBuild model\u003c/li\u003e\n","                \u003cli\u003eVisualization of Decision Tree\u003c/li\u003e\n","            \u003c/ul\u003e\n","\u003c/ol\u003e\n","\n","\u003c/div\u003e\n","\n","----\n"]},{"cell_type":"markdown","id":"b66559da-f9a0-440f-8bbf-dbabfc93ad74","metadata":{},"outputs":[],"source":["## Dataset Description\n","\n","### Context\n","Dataset contains historical changes of the ***BTC/BUSD*** and ***ADOSC, NATR, TRANGE indicators*** for the period from *11/11/2022 to 11/24/2022* with an *1-minute* aggregation time.\n","\n","### Columns\n","\n","#### Input columns\n","* ***Time*** - the timestamp of the record\n","* ***Open*** -  the price of the asset at the beginning of the trading period\n","* ***High*** -  the highest price of the asset during the trading period\n","* ***Low*** - the lowest price of the asset during the trading period.\n","* ***Close*** - the price of the asset at the end of the trading period\n","* ***Volume*** - the total number of shares or contracts of a particular asset that are traded during a given period\n","* ***Count*** -  the number of individual trades or transactions that have been executed during a given time period\n","* ***ADOSC*** - Chaikin oscillator indicator\n","* ***NATR*** - normalized average true range (ATR) indicator\n","* ***TRANGE*** - true range indicator\n","* ***Volume_binned*** - categorical field that indicates the size of the Volume *(Low, Medium, High)*\n","* ***ADOSC_binned*** - categorical field that indicates the size of the ADOSC indicator *(Low, Medium, High)*\n","* ***NATR_binned*** - categorical field that indicates the size of the NATR indicator *(Low, Medium, High*\n","* ***TRANGE_binned*** - categorical field that indicates the size of the TRANGE indicator *(Low, Medium, High)*\n","\n","#### Target columns\n","* ***Price*** - the average price at which a particular asset has been bought or sold during a given period\n","* ***Price_binned*** - categorical field that indicates the size of the Price *(Low, Medium, High)*\n"]},{"cell_type":"markdown","id":"15351b7e-1372-40f6-975e-2ddf23304531","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"88964b7d-dbed-4eda-a7f6-b91c35da7b61","metadata":{},"outputs":[],"source":["During the work, the task of a preliminary analysis of cryptocurrency price level based on numerical indicator values and its division into categories by levels.\n","\n","In this lesson, we will try to give answers to a set of questions that may be relevant when analyzing banking data:\n","\n","1. What are the most useful Python libraries for classification analysis?\n","2. How to transform category data?\n","3. How to create DataSet?\n","4. How to do features selection?\n","5. How to make, fit and visualize classification model?\n","\n","In addition, we will make the conclusions for the obtained results of our classification analysis to discover wether indicators can be used in cryptocurrency price prediciton.\n"]},{"cell_type":"markdown","id":"84804910-d029-4138-883d-7bfb56530310","metadata":{},"outputs":[],"source":["## 1. Import and Load Data\n"]},{"cell_type":"markdown","id":"79f8fe8e-c3c7-4ff4-8822-8dcb44541491","metadata":{},"outputs":[],"source":["### Setup\n","\n","[Scikit-learn](https://scikit-learn.org/stable/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01) (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n","\n","Let's install \u003cem\u003escikit-learn\u003c/em\u003e and other needed modules:\n"]},{"cell_type":"code","id":"85b9d8ca-4ad5-42ae-af21-ffd9257c855e","metadata":{},"outputs":[],"source":["! conda install -c conda-forge scikit-learn -y\n! conda install python-graphviz -y"]},{"cell_type":"markdown","id":"05f20c18-edcf-42a2-91b0-cbfb6e023f9f","metadata":{},"outputs":[],"source":["### Import Libraries\n"]},{"cell_type":"markdown","id":"cdd82cbc-deff-4ec2-86d0-2e44f27992b6","metadata":{},"outputs":[],"source":["Import the libraries necessary to use in this lab. We can add some aliases to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings.\n"]},{"cell_type":"code","id":"b600457e-fafe-4a1f-ba87-d31d29162117","metadata":{},"outputs":[],"source":["import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport graphviz\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (8, 6)\n# Data transformation\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n# Features Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, mutual_info_classif\n# Classificators\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn import tree\n# warnings deactivate\nimport warnings\nwarnings.filterwarnings('ignore')\n# for better visualization\nfrom sklearn import set_config\nset_config(display = 'diagram')"]},{"cell_type":"markdown","id":"1f0ccf9b-813e-4ea9-aaf0-d105cf1426e2","metadata":{},"outputs":[],"source":["Further specify the value of the `precision` parameter equal to 2 to display two decimal signs (instead of 6 as default).\n"]},{"cell_type":"code","id":"69633e15-33e2-4ac9-b452-a396bd15f697","metadata":{},"outputs":[],"source":["pd.set_option(\"precision\", 2)\npd.options.display.float_format = '{:.2f}'.format"]},{"cell_type":"markdown","id":"e132a798-7a94-4700-b64e-27a1b55dce01","metadata":{},"outputs":[],"source":["### Load Data\n"]},{"cell_type":"markdown","id":"40bd3132-c318-4bb0-9458-fcb19d9e838c","metadata":{},"outputs":[],"source":["We will use the same DataSet like in previous labs. Therefore next some steps will be the same.\n"]},{"cell_type":"markdown","id":"3d6257f6-41d5-4128-8d86-16a5c6756d58","metadata":{},"outputs":[],"source":["First, we assign the URL of the dataset to \u003ccode\u003e\"path\"\u003c/code\u003e. \n"]},{"cell_type":"code","id":"2653a538-1ca1-4ac0-8137-b64dc78f5b33","metadata":{},"outputs":[],"source":["path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0ZXSEN/BTCBUSD_trade.csv' "]},{"cell_type":"markdown","id":"31facdea-73c0-4c80-9ac7-02158dbffc1c","metadata":{},"outputs":[],"source":["Then use the Pandas method \u003ccode\u003eread_csv()\u003c/code\u003e to load the data from the web address and set dataframe index column type to \u003cstrong\u003edatetime\u003c/strong\u003e using \u003ccode\u003epd.to_datetime()\u003c/code\u003e method for correct time series analysis. \n"]},{"cell_type":"code","id":"3f9d6677-c846-483e-98a3-ed645a1c5129","metadata":{},"outputs":[],"source":["df = pd.read_csv(path)\ndf.set_index('Time', inplace=True)\ndf.index = pd.to_datetime(df.index)\n\ndf.head()"]},{"cell_type":"code","id":"0cc41183-f2c5-46df-97eb-ac91cf544ce8","metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","id":"968e0275-62ae-4a13-8eb9-c7ca523979ff","metadata":{},"outputs":[],"source":["As you can see dataset consists of 15 columns. 'Price' column will be the target in further classification implementation. Also dataset has 16751 rows. In previous labs we investigated input columns. In our classification models we will use the following features:\n"]},{"cell_type":"markdown","id":"39d969e4-9108-4413-b127-186730dcf4f2","metadata":{},"outputs":[],"source":["Input features (column names):\n","1. `Volume` - the total number of units of the asset traded on all exchanges within a particular time period \u003cem\u003e(numeric)\u003c/em\u003e\n","2. `ADOSC` - an volume-based indicator to measure the cumulative flow of money into and out of an asset \u003cem\u003e(numeric)\u003c/em\u003e\n","3. `NATR` - an indicator measuring the volatility level \u003cem\u003e(numeric)\u003c/em\u003e\n","4. `TRANGE` - a technical indicator which measures the daily range plus any gap from the closing price of the preceding day\u003cem\u003e(numeric)\u003c/em\u003e\n","5. `Volume_binned` - Volume values divided into three category based on their level \u003cem\u003e(categorical: `Low`, `Medium`, `High`)\u003c/em\u003e\n","6. `ADOSC_binned` - ADOSC values divided into three category based on their level \u003cem\u003e(categorical: `Low`, `Medium`, `High`)\u003c/em\u003e\n","7. `NATR_binned` - NATR values divided into three category based on their level \u003cem\u003e(categorical: `Low`, `Medium`, `High`)\u003c/em\u003e\n","8. `TRANGE_binned` - TRANGE values divided into three category based on their level \u003cem\u003e(categorical: `Low`, `Medium`, `High`)\u003c/em\u003e\n","\n","Output feature (desired target):\n","\n","1. `Price_binned` - determine in which price category cryptocuttency price will be\n"]},{"cell_type":"markdown","id":"a6c69259-711e-4b73-aaf0-fc27ad7cb21c","metadata":{},"outputs":[],"source":["Our goal is create the classification model that can predict  the cryptocurrency price level. To do this we must analize and prepare data for such type of model.\n"]},{"cell_type":"markdown","id":"d5c8251b-5965-4882-a20e-9c733038504d","metadata":{},"outputs":[],"source":["## 2. Data preparation\n"]},{"cell_type":"markdown","id":"edc535c7-5745-40e2-a467-1204b2e7dc05","metadata":{},"outputs":[],"source":["### Data transformation\n"]},{"cell_type":"markdown","id":"1d4eb862-f281-43a3-8c4a-62bb1c726e11","metadata":{},"outputs":[],"source":["First of all we should investigate how Pandas recognized types of features.\n"]},{"cell_type":"code","id":"174e2271-4965-4845-a3e0-8aeffd422a9a","metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","id":"2e6bb8c8-ea91-4169-be8d-838c8f24a96d","metadata":{},"outputs":[],"source":["As you can see all categorical features was recogized like object. We must change thair type on \"categorical\". \n"]},{"cell_type":"code","id":"95343a1c-7f93-4222-8b29-f1a265e86e62","metadata":{},"outputs":[],"source":["col_cat = list(df.select_dtypes(include=['object']).columns)\ncol_cat"]},{"cell_type":"markdown","id":"1a986f4f-75d9-400a-87a1-9a5eee101a1d","metadata":{},"outputs":[],"source":["Let's look at the dataset size.\n"]},{"cell_type":"code","id":"a28db06d-cc8c-46d8-891d-4007f60c1d5e","metadata":{},"outputs":[],"source":["df.loc[:, col_cat] = df[col_cat].astype('category')\ndf.info()"]},{"cell_type":"markdown","id":"2fbf3b9d-678c-408d-aa0a-6828efc99b35","metadata":{},"outputs":[],"source":["To see the unique values of exact feature (column) we can use \u003ccode\u003eunique()\u003c/code\u003e function:\n"]},{"cell_type":"code","id":"274e3cf7-da77-4385-967a-5d24d0abc3df","metadata":{},"outputs":[],"source":["df['ADOSC_binned'].unique()"]},{"cell_type":"markdown","id":"596b7084-5be8-41d1-a374-249bae62f978","metadata":{},"outputs":[],"source":["As was signed earlier the dataset contains 16571 objects (rows), for each of which 15 features are set (columns), including 1 target feature (y). 5 features, including target are categorical. These data type of values cannot use for classification. We must transform it to int or float. \n","To do this we can use **[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)** and **[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)**. These functions can encode categorical features as an integer array.\n","\n","Firs of all we separate DataSet on input and output (target) DataSets:\n"]},{"cell_type":"code","id":"bbabbc25-482e-46c0-91a6-65041ea2a644","metadata":{},"outputs":[],"source":["X = df[['Volume', 'ADOSC', 'NATR', 'TRANGE', 'Volume_binned', 'ADOSC_binned', 'NATR_binned', 'TRANGE_binned']]  #input columns\ny = df['Price_binned']    #target column "]},{"cell_type":"markdown","id":"365498e8-1e8c-4c62-b57e-161d791a4366","metadata":{},"outputs":[],"source":["### Encoding and Normalization\n"]},{"cell_type":"markdown","id":"289daa24-8317-449d-97f2-1007960b4e6b","metadata":{},"outputs":[],"source":["Than create list of categorical fields and transform thair values to int arrays:\n"]},{"cell_type":"code","id":"7708a799-c0e4-4f91-b642-7cfeec8fe26e","metadata":{},"outputs":[],"source":["col_cat = ['Volume_binned', 'ADOSC_binned', 'NATR_binned', 'TRANGE_binned']\noe = OrdinalEncoder()\noe.fit(X[col_cat])\nX_cat_enc = oe.transform(X[col_cat])"]},{"cell_type":"code","id":"917dc847-cbb3-4d7b-8353-37c9d012dd52","metadata":{},"outputs":[],"source":["X_cat_enc"]},{"cell_type":"markdown","id":"952a696a-0238-4ac5-b788-598fbe222ce1","metadata":{},"outputs":[],"source":["Than we must transform arrays back into DataFrame:\n"]},{"cell_type":"code","id":"05063d39-0843-47eb-8d2a-939afc7a2e81","metadata":{},"outputs":[],"source":["X_cat_enc = pd.DataFrame(X_cat_enc)\nX_cat_enc.columns = col_cat\nX_cat_enc"]},{"cell_type":"markdown","id":"29437206-2425-425f-bcc4-a80608b51f23","metadata":{},"outputs":[],"source":["Numerical fields can have different scale and can consists negative values. These will lead to round mistakes and exeptions for some AI methods. To avoid it these features must be normalized.\n","\n","Let's create list of numerical fields and normilize it using by **[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)**:\n"]},{"cell_type":"code","id":"ec75ab30-2bc6-4268-8b9b-9a005cf92124","metadata":{},"outputs":[],"source":["col_num = ['Volume', 'ADOSC', 'NATR', 'TRANGE']\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_num_enc = scaler.fit_transform(X[col_num])"]},{"cell_type":"code","id":"c8eaf52f-6172-4100-8183-47fb0ebee92e","metadata":{},"outputs":[],"source":["X_num_enc"]},{"cell_type":"markdown","id":"a257ec45-cdf9-44e4-8cf2-c5e684ac65e1","metadata":{},"outputs":[],"source":["Like in previous case transform back obtained arrays into DataFrame:\n"]},{"cell_type":"code","id":"f42c0fc6-bf02-4e01-bcfc-d6d27f6c3ab0","metadata":{},"outputs":[],"source":["X_num_enc = pd.DataFrame(X_num_enc)\nX_num_enc.columns = col_num\nX_num_enc"]},{"cell_type":"markdown","id":"687dd5e7-a271-4109-81e1-3d65aafe2e30","metadata":{},"outputs":[],"source":["Then we should concatenate these DataFrames in one input DataFrame:\n"]},{"cell_type":"code","id":"3c554843-3af6-4224-8a1b-6e2bf340dd64","metadata":{},"outputs":[],"source":["x_enc = pd.concat([X_cat_enc, X_num_enc], axis=1)\nx_enc"]},{"cell_type":"markdown","id":"0b3254e7-ad1f-4460-bcb7-4851cae92c5d","metadata":{},"outputs":[],"source":["The same transformation we must do for target field:\n"]},{"cell_type":"code","id":"d11275e7-2f3f-494d-abb7-72229a6b1f31","metadata":{},"outputs":[],"source":["le = LabelEncoder()\nle.fit(y)\ny_enc = le.transform(y)\ny_enc = pd.Series(y_enc)\ny_enc.columns = y.name"]},{"cell_type":"code","id":"871f467d-b674-4186-8d15-20c29bb92503","metadata":{},"outputs":[],"source":["y.to_frame()"]},{"cell_type":"code","id":"a7695404-92ff-46f5-a68b-9a856c66b9d9","metadata":{},"outputs":[],"source":["y_enc.unique()"]},{"cell_type":"markdown","id":"f261fe1f-108a-4be7-a482-8f75d12a64af","metadata":{},"outputs":[],"source":["As you can see values \u003cem\u003e'High'\u003c/em\u003e was changed on 0, \u003cem\u003e'Low'\u003c/em\u003e on 1, and \u003cem\u003e'Medium'\u003c/em\u003e on 2.\n"]},{"cell_type":"markdown","id":"1c0aed36-2829-4425-8872-5663645d1a99","metadata":{},"outputs":[],"source":["## 3. Features selection\n"]},{"cell_type":"markdown","id":"3ee336fd-d941-43bd-854f-c1b292bb401f","metadata":{},"outputs":[],"source":["As was signed before input fields consists 8 features. Of coure some of them are more significant for classification. \n","\n","There are two popular feature selection techniques that can be used for categorical input data and a categorical (class) target variable:\n","* Chi-Squared Statistic.\n","* Mutual Information Statistic.\n","\n","Let’s take a closer look at each in turn. To do this we can use **[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)**.\n"]},{"cell_type":"markdown","id":"d9e5fa7f-3e04-4e4b-bce7-c2dbc8afe914","metadata":{},"outputs":[],"source":["### Chi-Squared Statistic\n"]},{"cell_type":"markdown","id":"a5e60382-7a62-404e-ba9e-403dc8d4460d","metadata":{},"outputs":[],"source":["\u003cem\u003e\u003cstrong\u003ePearson’s chi-squared statistical hypothesis test\u003c/strong\u003e\u003c/em\u003e is an example of a test for independence between categorical variables.\n","\n","You can learn more about this statistical test in the tutorial:\n","*   [A Gentle Introduction to the Chi-Squared Test for Machine Learning](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01).\n","\n","The results of this test can be used for feature selection, where those features that are independent of the target variable can be removed from the dataset.\n","\n","The scikit-learn machine library provides an implementation of the chi-squared test in the **[chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01#sklearn.feature_selection.chi2)** function. This function can be used in a feature selection strategy, such as selecting the top k most relevant features (largest values) via the SelectKBest class.\n","\n","For example, we can define the \u003cem\u003eSelectKBest class\u003c/em\u003e to use the \u003ccode\u003echi2()\u003c/code\u003e function and select all (or most significant) features, then transform the train and test sets.\n"]},{"cell_type":"markdown","id":"66048b58-d03f-4938-974e-392bf7f974c5","metadata":{},"outputs":[],"source":["Apply SelectKBest class to extract features:\n"]},{"cell_type":"code","id":"74d21276-0f13-4687-b0c9-9613a0697c91","metadata":{},"outputs":[],"source":["bestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(x_enc,y_enc)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)"]},{"cell_type":"markdown","id":"e7aa76fc-5656-4a69-ba62-8645a1bf0f6d","metadata":{},"outputs":[],"source":["Concat two dataframes for better visualization:\n"]},{"cell_type":"code","id":"40de7469-1ef6-498a-bb1c-5479d32cfac3","metadata":{},"outputs":[],"source":["featureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nfeatureScores.sort_values(by=['Score'], ascending=False)"]},{"cell_type":"markdown","id":"e1c520b3-a1af-439b-af33-ce78689da929","metadata":{},"outputs":[],"source":["### Mutual Information Statistic \n"]},{"cell_type":"markdown","id":"6fa55468-b758-4e70-8e97-d703e809f959","metadata":{},"outputs":[],"source":["\u003cem\u003e\u003cstrong\u003eMutual information\u003c/strong\u003e\u003c/em\u003e from the field of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection.\n","\n","\u003cem\u003eMutual information\u003cem\u003e is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.\n","\n","[You can learn more about mutual information in the following tutorial.](https://machinelearningmastery.com/information-gain-and-mutual-information?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)\n","\n","The scikit-learn machine learning library provides an implementation of mutual information for feature selection via the **[mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01#sklearn.feature_selection.mutual_info_classif)** function.\n","\n","Like \u003ccode\u003echi2()\u003c/code\u003e, it can be used in the SelectKBest feature selection strategy (and other strategies).\n"]},{"cell_type":"code","id":"f04112a5-80e1-4774-8afa-745237279587","metadata":{},"outputs":[],"source":["bestfeatures = SelectKBest(score_func=mutual_info_classif, k='all')\nfit = bestfeatures.fit(x_enc,y_enc)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nfeatureScores.sort_values(by=['Score'], ascending=False)"]},{"cell_type":"markdown","id":"8af117c4-4fb9-461a-9972-6dfb3e7c6b5f","metadata":{},"outputs":[],"source":["As you can see these 2 function select almost the same significant features.\n"]},{"cell_type":"markdown","id":"28f046f0-3221-478d-9d49-eb423a385500","metadata":{},"outputs":[],"source":["We can see that categorical dataframe columns have the most significant impact. Thus, let's consider only them as inputs for predicting model.  \n"]},{"cell_type":"code","id":"899abad9-a6ef-4c96-98a7-3cba315152fa","metadata":{},"outputs":[],"source":["x_enc = x_enc[x_enc.columns[:4]]\nx_enc"]},{"cell_type":"markdown","id":"83a9406a-b558-4c35-8f13-ee131110e7a2","metadata":{},"outputs":[],"source":["### Feature Importance\n"]},{"cell_type":"markdown","id":"2f15ec4e-9b3f-41ec-aceb-2ff980fa9880","metadata":{},"outputs":[],"source":["You can get the feature importance of each feature of your DataFrame by using the feature importance property of the exact classification model.\n","\n","\u003cem\u003eFeature importance\u003c/em\u003e gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n","\n","\u003cem\u003e\u003cstrong\u003eFor example:\u003c/strong\u003e\u003c/em\u003e\n","\n","Feature importance is an inbuilt class that comes with **[Tree Based Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)**, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.\n"]},{"cell_type":"markdown","id":"75c9e859-504d-41df-81f8-de987d35e74f","metadata":{},"outputs":[],"source":["Let's create and fit the model:\n"]},{"cell_type":"code","id":"b0dedab8-5f31-4277-b206-4f8f9e1ed29f","metadata":{},"outputs":[],"source":["model = ExtraTreesClassifier()\nmodel.fit(x_enc,y_enc)"]},{"cell_type":"markdown","id":"0c9e78d1-9bbf-4f65-8b7c-c3ed867e0960","metadata":{},"outputs":[],"source":["Use inbuilt \u003ccode\u003efeature_importances\u003c/code\u003e method of tree based classifiers:\n"]},{"cell_type":"code","id":"848fba34-57a7-4980-b2ac-21cd1942328d","metadata":{},"outputs":[],"source":["print(model.feature_importances_)"]},{"cell_type":"markdown","id":"b77e1977-4ee0-4cfe-ba88-6fab652ee6de","metadata":{},"outputs":[],"source":["Let's transform it into Series and plot graph of feature importances for better visualization:\n"]},{"cell_type":"code","id":"4782809e-4d18-48a0-b68c-94b9d6ed78a9","metadata":{},"outputs":[],"source":["feat_importances = pd.Series(model.feature_importances_, index=x_enc.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()"]},{"cell_type":"markdown","id":"db3d49d0-bbc5-4bff-8c97-cb539a6f280a","metadata":{},"outputs":[],"source":["You can see that for \u003cem\u003eExtra Tree Classifier\u003c/em\u003e impotance of features are the same as in previous cases. \n"]},{"cell_type":"markdown","id":"e4159da1-79ad-47f3-8642-7c3e2c030cd7","metadata":{},"outputs":[],"source":["## 4. Classification models\n"]},{"cell_type":"markdown","id":"a242ee8e-11b3-4622-990f-bac7871db816","metadata":{},"outputs":[],"source":["### Train and Test DataSets Creation\n"]},{"cell_type":"markdown","id":"728434d9-5c60-4a73-a95a-51f83356ba33","metadata":{},"outputs":[],"source":["First of all we must separate DataSets for train and test DataSets for calculate accuracy of models. To do this we can use **[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)**. \n","\n","Let's separate DataSets in \u003cem\u003e0.33\u003c/em\u003e proportion \u003cem\u003etrain/test\u003cem\u003e:\n"]},{"cell_type":"code","id":"44c57fc3-088f-43ea-99d0-96b974c666ba","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(x_enc, y_enc, test_size=0.33, shuffle=False, random_state=1)\nprint(\"X_train:\", X_train.shape)\nprint(\"X_test:\", X_test.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"y_test:\", y_test.shape)"]},{"cell_type":"markdown","id":"fc7db420-cce6-42c3-b252-1f3b4a6d9e5d","metadata":{},"outputs":[],"source":["### Extra Trees Classifier\n"]},{"cell_type":"markdown","id":"557a03d5-7169-415a-a28e-1a1be513200f","metadata":{},"outputs":[],"source":["#### What is an extra tree classifier?\n","\n","\u003cstrong\u003e\u003cem\u003eExtra trees\u003c/em\u003e\u003c/strong\u003e (short for extremely randomized trees)\u003c/em\u003e is an ensemble supervised machine learning method that uses decision trees and perform their averaging to improve the predictive accuracy and control overfitting.\n","\n","Let's create and fit ExtraTreesClassifier on our train dataset and calculate accuracy of classification:\n"]},{"cell_type":"code","id":"5c6e23d3-afc6-4193-be39-162654cec8d8","metadata":{},"outputs":[],"source":["model = ExtraTreesClassifier()\nmodel.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"e3e8eb34-8114-45c0-b7fe-65e777e16a5c","metadata":{},"outputs":[],"source":["Evaluate the model on test data for obtain predictions:\n"]},{"cell_type":"code","id":"fce2c46e-13b6-4f5d-adb5-739f06e2797b","metadata":{},"outputs":[],"source":["yhat = model.predict(X_test)\nprint(yhat)"]},{"cell_type":"markdown","id":"5e5298e3-22e8-46bd-b949-b211edcd8c0b","metadata":{},"outputs":[],"source":["Evaluate accuracy: \n"]},{"cell_type":"code","id":"ce540e31-9cb3-4b69-aafa-264e95484463","metadata":{},"outputs":[],"source":["accuracy = accuracy_score(y_test, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"]},{"cell_type":"markdown","id":"990c9b69-e223-4922-87f7-7ffbe7690d33","metadata":{},"outputs":[],"source":["There are many different techniques for scoring features and selecting features based on scores. \u003cem\u003eHow do you know which one to use?\u003c/em\u003e\n","\n","A robust approach is to evaluate models using different feature selection methods (and numbers of features) and select the method that results in a model with the best performance.\n"]},{"cell_type":"markdown","id":"1bd432dc-6474-4226-985d-0867d1fd5fa8","metadata":{},"outputs":[],"source":["### Logistic Regression\n"]},{"cell_type":"markdown","id":"9f9a19c5-e4db-4b5b-ae4d-76b26e5123bd","metadata":{},"outputs":[],"source":["\u003cem\u003e\u003cstrong\u003eLogistic regression\u003c/strong\u003e\u003c/em\u003e is a good model for testing feature selection methods as it can perform better if irrelevant features are removed from the model. We will use this model in absolutelly similar way like previous one.\n"]},{"cell_type":"code","id":"5ec44c0a-f81e-40e1-a1fb-67c25e0bad62","metadata":{},"outputs":[],"source":["model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_train, y_train)\nyhat = model.predict(X_test)\naccuracy = accuracy_score(y_test, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"]},{"cell_type":"markdown","id":"2695d71f-b80a-4552-9c67-4d3f549303f9","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick \u003cb\u003ehere\u003c/b\u003e for the solution\u003c/summary\u003e \n","\u003ccode\u003e    \n","model = LogisticRegression(solver='lbfgs')\n","model.fit(X_train, y_train)\n","yhat = model.predict(X_test)\n","accuracy = accuracy_score(y_test, yhat)\n","print('Accuracy: %.2f' % (accuracy*100))\n","    \u003c/code\u003e\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"f3db3b0a-d8ca-47e5-83f0-e52276f973a7","metadata":{},"outputs":[],"source":["As you can see accuracy did not improve.\n"]},{"cell_type":"markdown","id":"fa669853-bffc-4c9e-8739-7d9e72f3efdc","metadata":{},"outputs":[],"source":["## Decision Tree \n"]},{"cell_type":"markdown","id":"60a2d3e5-255d-4d36-8179-9d8780761044","metadata":{},"outputs":[],"source":["The biggest drawback of the previous methods is the inability to visualize or justify the decision.\n"]},{"cell_type":"markdown","id":"ac9972d9-b9d2-4072-a202-3793f41a3eaf","metadata":{},"outputs":[],"source":["\u003cstrong\u003e\u003cem\u003eDecision trees\u003c/em\u003e\u003c/strong\u003e are a popular supervised learning method for a variety of reasons. \n","\n","Benefits of decision trees include that \u003cem\u003ethey can be used for both regression and classification\u003c/em\u003e, they don’t require feature scaling, and they are relatively easy to interpret as you can visualize decision trees. This is not only a powerful way to understand your model, but also to communicate how your model works. \n","\n","Consequently, it would help to know how to make a visualization based on your model.\n"]},{"cell_type":"markdown","id":"36580f07-36ee-4f5e-b605-434485a06937","metadata":{},"outputs":[],"source":["\u003cem\u003eA [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)\u003c/em\u003e is a supervised algorithm used in machine learning. It is using a binary tree graph (each node has two children) to assign for each data sample a target value. The target values are presented in the tree leaves. To reach to the leaf, the sample is propagated through nodes, starting at the root node. In each node a decision is made, to which descendant node it should go. \n","\n","A decision is made based on the selected sample’s feature. Decision Tree learning is a process of finding the optimal rules in each internal tree node according to the selected metric.\n"]},{"cell_type":"markdown","id":"382d7741-5b35-4e24-9598-6303cae37be0","metadata":{},"outputs":[],"source":["### Build model\n"]},{"cell_type":"markdown","id":"ed17bf34-3e59-4324-9b67-fcb9ee3cc110","metadata":{},"outputs":[],"source":["This metod allows also to calculate features impotance.\n","Let's calculate them. Choice best 10 of them. Refit the model and visualize decision tree.\n"]},{"cell_type":"code","id":"69763994-aabb-4432-9fc5-42da4c5f92cb","metadata":{},"outputs":[],"source":["model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nyhat = model.predict(X_test)\naccuracy = accuracy_score(y_test, yhat)\nprint('Accuracy: %.2f' % (accuracy*100)) "]},{"cell_type":"code","id":"9dd1c510-a490-4c9a-8079-df735cad34dd","metadata":{},"outputs":[],"source":["print(\"Features impotance:\", model.feature_importances_)"]},{"cell_type":"markdown","id":"b4c7838a-0edd-494d-8e52-bca07e545892","metadata":{},"outputs":[],"source":["Plot graph of feature importances for better visualization\n"]},{"cell_type":"code","id":"af374ca0-aeac-4462-b4be-941b299c4457","metadata":{},"outputs":[],"source":["feat_importances = pd.Series(model.feature_importances_, index=x_enc.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()"]},{"cell_type":"markdown","id":"b237fc3c-d71e-4395-83fe-e0965456b161","metadata":{},"outputs":[],"source":["### Visualization of Decision Tree\n"]},{"cell_type":"markdown","id":"d9b71486-3843-4e2d-ae09-faf658aefbc5","metadata":{},"outputs":[],"source":["Let's visualize decision tree.\n","There are some ways to do it:\n","\n","*   Text visualization\n","*   Plot tree\n","*   Graph visualization\n"]},{"cell_type":"markdown","id":"56a91079-d16d-4254-a912-dbf3fd35db62","metadata":{},"outputs":[],"source":["### Text visualization\n"]},{"cell_type":"code","id":"ccc94a2e-d418-487f-a563-c97c740a503e","metadata":{},"outputs":[],"source":["text_representation = tree.export_text(model)\nprint(text_representation)"]},{"cell_type":"markdown","id":"fb91a7b5-b4ad-4cff-a6f7-37310a4a1ace","metadata":{},"outputs":[],"source":["You can save it into file:\n"]},{"cell_type":"code","id":"329d1783-d1cc-41ca-acb5-6970ea139a4c","metadata":{},"outputs":[],"source":["with open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)"]},{"cell_type":"markdown","id":"1de893ad-51ad-447d-9219-4a3eba3d9baa","metadata":{},"outputs":[],"source":["### Plot tree \n"]},{"cell_type":"markdown","id":"cf2753b0-7826-4a8e-bfb9-6467b1e95809","metadata":{},"outputs":[],"source":["You can plot tree using by two different way:\n","1. \u003ccode\u003e**[plot_tree()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX01KTEN2525-2023-01-01)**\u003c/code\u003e\n","2. \u003ccode\u003eexport_graphviz()\u003c/code\u003e from \u003cem\u003egraphviz library \u003cem\u003e\n"]},{"cell_type":"markdown","id":"eb6646c4-8e09-4627-96dd-5365d81f16c9","metadata":{},"outputs":[],"source":["Because of slow rendering \u003ccode\u003eplot_tree\u003c/code\u003e implementation can take some time: \n"]},{"cell_type":"code","id":"8e3fe703-f1f5-403c-80d5-e76f7401c460","metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model,\n               feature_names = x_enc.columns, \n               class_names = y.unique(),\n               filled = True)"]},{"cell_type":"code","id":"5fb7293b-341c-42b6-b2b6-73e8990caddc","metadata":{},"outputs":[],"source":["fig.savefig('decision_tree.png')"]},{"cell_type":"markdown","id":"a48354b7-4edb-483a-995c-8e789caac9d8","metadata":{},"outputs":[],"source":["Let's visualize our decision tree in a graph form using \u003cem\u003egraphviz\u003c/em\u003e module:\n"]},{"cell_type":"code","id":"e9bec452-edf9-4378-8337-6f79af65e704","metadata":{},"outputs":[],"source":["dot_data = tree.export_graphviz(model,\n               feature_names = x_enc.columns, \n               class_names = y.unique(),\n                                filled=True)"]},{"cell_type":"markdown","id":"15f91d4d-05aa-4fff-9888-be1c3ddaf644","metadata":{},"outputs":[],"source":["After creation you can draw graph:\n"]},{"cell_type":"code","id":"e8ffecb2-ca2d-412c-8480-e62cbe0cfdef","metadata":{},"outputs":[],"source":["graph = graphviz.Source(dot_data, format=\"png\") \ngraph"]},{"cell_type":"markdown","id":"52b70da2-adb4-4b38-bb27-b0282cae6c8a","metadata":{},"outputs":[],"source":["And render it into file:\n"]},{"cell_type":"code","id":"473b53c8-a24d-4212-a755-52f8971ba420","metadata":{},"outputs":[],"source":["graph.render(\"decision_tree_graphivz\")"]},{"cell_type":"markdown","id":"f6b409a9-95bc-4474-a2ea-c68b936cafa3","metadata":{},"outputs":[],"source":["## Conclusion\n"]},{"cell_type":"markdown","id":"9302c8c5-5357-42ec-b48e-a02de75fb4e3","metadata":{},"outputs":[],"source":["In this lab we learned to do preliminary data processing. In particular, change data types, normalize and process categorical data. It was shown how to make feature selection by different methods. Learned how to build training and test DataSets. Shows how to work with different classifiers. It was also shown how to visualize a decision tree.\n","\n","As a result, the accuracy of all three classification models did not exceed 52%. In the following courses, we will consider the effectiveness of wider range of nonlinear models in economics and financial services.\n"]},{"cell_type":"markdown","id":"e31186d7-653f-406a-91ea-2eaf59f8c749","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","\u003cb style=\"font-size: 2em; font-weight: 600;\"\u003eQuestion #1:\u003c/b\u003e\n","\n","Create an ExtraTreesClassifier object called \"model\".\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"6322f07f-27d5-4200-864f-3aff3a1263ec","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n\nmodel = ExtraTreesClassifier()"]},{"cell_type":"markdown","id":"e1a55e06-f7ff-40a5-a613-65eb7b19291b","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick \u003cb\u003ehere\u003c/b\u003e for the solution\u003c/summary\u003e \n","\u003ccode\u003emodel = ExtraTreesClassifier()\u003c/code\u003e\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"711f1801-5b03-4878-9407-2ca427031ad8","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","\u003cb style=\"font-size: 2em; font-weight: 600;\"\u003eQuestion #2:\u003c/b\u003e\n","\n","Create user function that will calculate accuracy of defined classificator model.\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"1d5560af-a1c3-4a08-b985-3dbbea90f44e","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n\ndef model_ac(x_train, y_train, x_test, y_test, clf):\n    model.fit(x_train, y_train)\n    yhat = model.predict(x_test)\n    accuracy = accuracy_score(y_test, yhat)\n    return accuracy"]},{"cell_type":"markdown","id":"079610eb-e121-4556-a4c1-c9c7b7642851","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick \u003cb\u003ehere\u003c/b\u003e for the solution\u003c/summary\u003e \n","\u003ccode\u003e\n","    model.fit(x_train, y_train)\n","    yhat = model.predict(x_test)\n","    accuracy = accuracy_score(y_test, yhat)\n","    return accuracy\n","\u003c/code\u003e\n","\u003c/details\u003e\n"]},{"cell_type":"code","id":"e7dd43bd-496f-4344-a00a-af8b0563b659","metadata":{},"outputs":[],"source":["print('Accuracy: %.2f' % (model_ac(X_train, y_train, X_test, y_test, model)*100))"]},{"cell_type":"markdown","id":"243ea8e7-0e03-4633-99df-63e99326ede1","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","\u003cb style=\"font-size: 2em; font-weight: 600;\"\u003eQuestion #3:\u003c/b\u003e\n","\n","Create user function that will calculate features impotance of defined classificator model.\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"a87d5b7e-fd0b-403a-a572-08d47df3b777","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n\ndef model_imp(x_train, y_train, clf):\n    model.fit(x_train, y_train)\n    feat_importances = pd.Series(model.feature_importances_, index=x_enc.columns)\n    return feat_importances.sort_values(ascending=False)"]},{"cell_type":"markdown","id":"5f233edb-f281-4079-b51c-d5fd55241715","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick \u003cb\u003ehere\u003c/b\u003e for the solution\u003c/summary\u003e \n","\u003ccode\u003e    \n","    model.fit(x_train, y_train)\n","    feat_importances = pd.Series(model.feature_importances_, index=x_enc.columns)\n","    return feat_importances.sort_values(ascending=False)\n","    \u003c/code\u003e\n","\u003c/details\u003e\n"]},{"cell_type":"code","id":"550d2098-099c-4699-9dc8-ffe8b2556a56","metadata":{},"outputs":[],"source":["imp = model_imp(X_train, y_train, model)\nprint(imp)"]},{"cell_type":"markdown","id":"5046bd77-6c17-4d4f-a019-305de02b9c8b","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","\u003cb style=\"font-size: 2em; font-weight: 600;\"\u003eQuestion #4:\u003c/b\u003e\n","\n","Build plot that show accuracy of defined model depedence on numbers of input features.\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"1e9df622-79d4-49f2-aa12-f2eb44508326","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n\ncol = []\nac = []\nfor c in imp.index:\n    col.append(c)\n    ac.append(model_ac(X_train[col], y_train, X_test[col], y_test, model))\n    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\nac = pd.Series(ac)\nac.plot()"]},{"cell_type":"markdown","id":"590e47ad-38a5-4f0d-af6e-65528ad1e0fa","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick \u003cb\u003ehere\u003c/b\u003e for the solution\u003c/summary\u003e \n","\u003ccode\u003e    \n","col = []\n","ac = []\n","for c in imp.index:\n","    col.append(c)\n","    ac.append(model_ac(X_train[col], y_train, X_test[col], y_test, model))\n","    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\n","ac = pd.Series(ac)\n","ac.plot()\n","    \u003c/code\u003e\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"e84ecf35-a574-41f8-bee9-de67dc3d449c","metadata":{},"outputs":[],"source":["### Thank you for completing this lab!\n","\n","## Authors\n","\n","\u003ca href=\"https://author.skills.network/instructors/yaryna_beida?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0ZXSEN2580-2023-01-01\"\u003eYaryna Beida\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0ZXSEN2580-2023-01-01\"\u003eProf. Yaroslav Vyklyuk, DrSc, PhD\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/mariya_fleychuk?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0ZXSEN2580-2023-01-01\"\u003eProf. Mariya Fleychuk, DrSc, PhD\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"f3d4b9d4-e641-46c8-80a0-3f2861a2d620","metadata":{},"outputs":[],"source":[" \n"," ## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                            |\n","| ----------------- | ------- | ---------- | --------------------------------------------- |\n","|     2023-03-25    |   1.0   |Yaryna Beida| Lab created                                   |\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e © IBM Corporation 2023. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}